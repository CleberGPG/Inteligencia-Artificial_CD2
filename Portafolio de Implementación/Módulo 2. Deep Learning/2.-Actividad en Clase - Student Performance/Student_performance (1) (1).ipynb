{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxdub7uIvsM-"
      },
      "source": [
        "### Problem Statement\n",
        "You are a data scientist working for a school\n",
        "\n",
        "You are asked to predict the GPA of the current students based on the following provided data:\n",
        "\n",
        " 0   StudentID  int64  \n",
        " 1   Age    int64  \n",
        " 2   Gender int64  \n",
        " 3   Ethnicity  int64  \n",
        " 4   ParentalEducation  int64  \n",
        " 5   StudyTimeWeekly    float64\n",
        " 6   Absences   int64  \n",
        " 7   Tutoring   int64  \n",
        " 8   ParentalSupport    int64  \n",
        " 9   Extracurricular    int64  \n",
        " 10  Sports int64  \n",
        " 11  Music  int64  \n",
        " 12  Volunteering   int64  \n",
        " 13  GPA    float64\n",
        " 14  GradeClass float64\n",
        "\n",
        "The GPA is the Grade Point Average, typically ranges from 0.0 to 4.0 in most educational systems, with 4.0 representing an 'A' or excellent performance.\n",
        "\n",
        "The minimum passing GPA can vary by institution, but it's often around 2.0. This usually corresponds to a 'C' grade, which is considered satisfactory.\n",
        "\n",
        "You need to create a Deep Learning model capable to predict the GPA of a Student based on a set of provided features.\n",
        "The data provided represents 2,392 students.\n",
        "\n",
        "In this excersice you will be requested to create a total of three models and select the most performant one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnY4h4PivsM_"
      },
      "source": [
        "### 1) Import Libraries\n",
        "\n",
        "First let's import the following libraries, if there is any library that you need and is not in the list bellow feel free to include it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xVdlFHQLvsNA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sphkRbHgw8e5",
        "outputId": "dd8d4328-8c07-400a-f9d7-f01c0130de65"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW4eWhbAvsNA"
      },
      "source": [
        "### 2) Load Data\n",
        "\n",
        "- You will be provided with a cvs (comma separated value) file.\n",
        "- You will need to add that file into a pandas dataframe, you can use the following code as reference\n",
        "- The file will be available in canvas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"MyDrive\"\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUQxWMsaxgwd",
        "outputId": "63ba4cbc-ab5c-4238-c619-71a1493c6200"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "'1.2.1 SE Ejercicio de Planeación de Base de Datos.gsheet'\n",
            "'1.2.2Ejercicio de Modelacion.drawio'\n",
            "'2021-11-25 (2).png'\n",
            "'2021-11-25 (3).png'\n",
            "'2.2.6 Actividad Ejercicio Clase Modelo relacional a Modelo relacional.gsheet'\n",
            "'2.4.2 Actividad Ejercicio 2 Entidad relación a Modelo relacional.gdoc'\n",
            " 2.5.1_Práctica_SQL-1.docx\n",
            " 2.6.1_Práctica_Normalización.docx\n",
            " A01236390_1erPasaporte\n",
            " A01236390_LINUX.docx\n",
            " A3D1F470-7683-4F51-8826-FBD35A4EA835.jpeg\n",
            "'Actividad 2 Usabilidad y Jugabilidad.gdoc'\n",
            " Actividad41InterseccionConvexHull.gdoc\n",
            "'Actividad 5.3 Hill Climber, ILS, SA.gdoc'\n",
            "'Actividad de aprendizaje 4 (1).gdoc'\n",
            "'Actividad de aprendizaje 4.gdoc'\n",
            " Activity1.mp4\n",
            "'Analisis del Contexto y la Normatividad .gdoc'\n",
            " AnalisisyReporte.ipynb\n",
            "'Base de datos distribuidas.gdoc'\n",
            " C2AA989C-3C86-44BD-8F83-A1AC5E868F90.jpeg\n",
            " Cancion.gdoc\n",
            "'clase 18 (1).gdoc'\n",
            "'clase 18.gdoc'\n",
            "'clase 22.gdoc'\n",
            "'Colab Notebooks'\n",
            "'Copia_de_Sports_Analytics_y_EDA (3).ipynb'\n",
            "'CURP_PEGC030808HCLRLLA6 (1).pdf'\n",
            " CURP_PEGC030808HCLRLLA6.pdf\n",
            " DataModifications.ipynb\n",
            "'Discurso individual Guion.gdoc'\n",
            "'Diseño de Prueba.gsheet'\n",
            "'Diseño inicial web del proyecto integrador.gslides'\n",
            "'DocumentoFinalClubAmanecer (1).pdf'\n",
            "'Documento sin título (1).gdoc'\n",
            "'Documento sin título (2).gdoc'\n",
            "'Documento sin título (3).gdoc'\n",
            "'Documento sin título (4).gdoc'\n",
            "'Documento sin título (5).gdoc'\n",
            "'Documento sin título (6).gdoc'\n",
            "'Documento sin título (7).gdoc'\n",
            "'Documento sin título.gdoc'\n",
            "'Ecuacion en geogebra.mp4'\n",
            "'Ejercicio MongoDB.gdoc'\n",
            "'Entrega 0: ChatBot.gdoc'\n",
            " EntregaProyecto.unitypackage\n",
            " EntregaProyecto.zip\n",
            "'evidencia 1 2.docx'\n",
            "'Evidencia entrega proyecto final Construccion de software'\n",
            "'Evidencia Integradora final: Plan de desarrollo personal.gslides'\n",
            "'Experimento1 FJ22.docx'\n",
            " Exposicion\n",
            " Exposiciones\n",
            " FrameworkSolucion.ipynb\n",
            "'General System Design (GSD) Checklist Example.doc'\n",
            "'Guion Debate.gdoc'\n",
            "'Historias de Usuarios.xlsx'\n",
            "'Hoja de cálculo sin título.gsheet'\n",
            " IMG_0052.MOV\n",
            "'Investigación: Cena de filósofos.docx'\n",
            " iris.data\n",
            "'Manual de Usuario y Administrador.gdoc'\n",
            "'Mapa de Empatía template.pptx'\n",
            "'Medicion de tiro Parabolico utilizando tracker.gdoc'\n",
            "'Modelacion en 3D.mp4'\n",
            "'My project_1.6 2.zip'\n",
            "'My project_1.6.zip'\n",
            "'Optimización de Consultas.gdoc'\n",
            "'Plantilla video reel Instagram.docx'\n",
            "'Plantilla video reel Instagram.gdoc'\n",
            "'Preguntas Cultura General.gdoc'\n",
            "'Programación Orientada a Objetos (Grupo 856).gdoc'\n",
            "\"Programming Language Master's Degree.gslides\"\n",
            " ProyectoFinal\n",
            " RandomWalks.gdoc\n",
            "'Reporte de Reto.docx'\n",
            "'Reporte de Semana 5.gdoc'\n",
            "'Reporte de semana 9.gdoc'\n",
            "'Reporte de Status semana 3.gdoc'\n",
            "'Reporte de status semana 7.gdoc'\n",
            "'Reto dielectroforesis .gslides'\n",
            "'Screen Recording 2023-04-13 at 13.27.08.mov'\n",
            "'SEQUENTIAL LOGIC.docx'\n",
            "'si CITA PARA PRESENTACION DE PROPUESTA DE ESENARIO PROPUESTO EN EQUIPO (1).xltm.gsheet'\n",
            "'si CITA PARA PRESENTACION DE PROPUESTA DE ESENARIO PROPUESTO EN EQUIPO.xltm.gsheet'\n",
            "'Situacion Problema 1.gsheet'\n",
            "'Situación problema F1008.gslides'\n",
            "'Student_performance_data _.csv'\n",
            "'Tarea 1 calidad.xlsx'\n",
            " TC3006_7C_101_E5\n",
            " TC3006_7C_101_E5-20240813T002747Z-001.zip\n",
            "'Ted Talk..gdoc'\n",
            "'THE DREAMWORLD.gdoc'\n",
            " Untitled\n",
            " Untitled0.ipynb\n",
            "'Untitled (1)'\n",
            "'Untitled (2)'\n",
            "'Untitled video - Made with Clipchamp (1).mp4'\n",
            " Valhalla23.csv\n",
            "'Variable Continua.gdoc'\n",
            "'Variable Discreta.gdoc'\n",
            "'Vectorización TF-IDF.gdoc'\n",
            " wine.data\n",
            " wine.names\n",
            " winequality-red.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "oUVv6apmvsNA",
        "outputId": "22667298-e30d-44a2-8ee2-115848bc7294"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      StudentID  Age  Gender  Ethnicity  ParentalEducation  StudyTimeWeekly  \\\n",
              "0          1001   17       1          0                  2        19.833723   \n",
              "1          1002   18       0          0                  1        15.408756   \n",
              "2          1003   15       0          2                  3         4.210570   \n",
              "3          1004   17       1          0                  3        10.028829   \n",
              "4          1005   17       1          0                  2         4.672495   \n",
              "...         ...  ...     ...        ...                ...              ...   \n",
              "2387       3388   18       1          0                  3        10.680555   \n",
              "2388       3389   17       0          0                  1         7.583217   \n",
              "2389       3390   16       1          0                  2         6.805500   \n",
              "2390       3391   16       1          1                  0        12.416653   \n",
              "2391       3392   16       1          0                  2        17.819907   \n",
              "\n",
              "      Absences  Tutoring  ParentalSupport  Extracurricular  Sports  Music  \\\n",
              "0            7         1                2                0       0      1   \n",
              "1            0         0                1                0       0      0   \n",
              "2           26         0                2                0       0      0   \n",
              "3           14         0                3                1       0      0   \n",
              "4           17         1                3                0       0      0   \n",
              "...        ...       ...              ...              ...     ...    ...   \n",
              "2387         2         0                4                1       0      0   \n",
              "2388         4         1                4                0       1      0   \n",
              "2389        20         0                2                0       0      0   \n",
              "2390        17         0                2                0       1      1   \n",
              "2391        13         0                2                0       0      0   \n",
              "\n",
              "      Volunteering       GPA  GradeClass  \n",
              "0                0  2.929196         2.0  \n",
              "1                0  3.042915         1.0  \n",
              "2                0  0.112602         4.0  \n",
              "3                0  2.054218         3.0  \n",
              "4                0  1.288061         4.0  \n",
              "...            ...       ...         ...  \n",
              "2387             0  3.455509         0.0  \n",
              "2388             0  3.279150         4.0  \n",
              "2389             1  1.142333         2.0  \n",
              "2390             0  1.803297         1.0  \n",
              "2391             1  2.140014         1.0  \n",
              "\n",
              "[2392 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-faeb29c3-9441-49d0-8406-409627d6a186\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudentID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>ParentalEducation</th>\n",
              "      <th>StudyTimeWeekly</th>\n",
              "      <th>Absences</th>\n",
              "      <th>Tutoring</th>\n",
              "      <th>ParentalSupport</th>\n",
              "      <th>Extracurricular</th>\n",
              "      <th>Sports</th>\n",
              "      <th>Music</th>\n",
              "      <th>Volunteering</th>\n",
              "      <th>GPA</th>\n",
              "      <th>GradeClass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>19.833723</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.929196</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15.408756</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.042915</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1003</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4.210570</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.112602</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>10.028829</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.054218</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1005</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4.672495</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.288061</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2387</th>\n",
              "      <td>3388</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>10.680555</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.455509</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2388</th>\n",
              "      <td>3389</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.583217</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.279150</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>3390</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6.805500</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.142333</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2390</th>\n",
              "      <td>3391</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12.416653</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.803297</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2391</th>\n",
              "      <td>3392</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>17.819907</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.140014</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2392 rows × 15 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faeb29c3-9441-49d0-8406-409627d6a186')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-faeb29c3-9441-49d0-8406-409627d6a186 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-faeb29c3-9441-49d0-8406-409627d6a186');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8750a0b5-2ebc-4922-b743-305614115f8d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8750a0b5-2ebc-4922-b743-305614115f8d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8750a0b5-2ebc-4922-b743-305614115f8d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7188b93c-1941-43d0-9bf6-965facdf73b1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7188b93c-1941-43d0-9bf6-965facdf73b1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 2392,\n  \"fields\": [\n    {\n      \"column\": \"StudentID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 690,\n        \"min\": 1001,\n        \"max\": 3392,\n        \"num_unique_values\": 2392,\n        \"samples\": [\n          2005,\n          1197,\n          3343\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 15,\n        \"max\": 18,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          18,\n          16,\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ethnicity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ParentalEducation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"StudyTimeWeekly\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.65277423586027,\n        \"min\": 0.001056538645936,\n        \"max\": 19.97809399526153,\n        \"num_unique_values\": 2392,\n        \"samples\": [\n          0.1357634804717955,\n          1.9899245236127627\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Absences\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 29,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          18,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tutoring\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ParentalSupport\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Extracurricular\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sports\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Music\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volunteering\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GPA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.91515582032496,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 2371,\n        \"samples\": [\n          3.3104012689001965,\n          3.4577117259752934\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GradeClass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2339075602260843,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "data = pd.read_csv(\"Student_performance_data _.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEyKq1VovsNB"
      },
      "source": [
        "### 3) Review you data:\n",
        "\n",
        "Make sure you review your data.\n",
        "Place special attention of null or empty values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Utt8p-33vsNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bde0bc0-8a30-4881-b3e0-c3764eaf5ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2392 entries, 0 to 2391\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   StudentID          2392 non-null   int64  \n",
            " 1   Age                2392 non-null   int64  \n",
            " 2   Gender             2392 non-null   int64  \n",
            " 3   Ethnicity          2392 non-null   int64  \n",
            " 4   ParentalEducation  2392 non-null   int64  \n",
            " 5   StudyTimeWeekly    2392 non-null   float64\n",
            " 6   Absences           2392 non-null   int64  \n",
            " 7   Tutoring           2392 non-null   int64  \n",
            " 8   ParentalSupport    2392 non-null   int64  \n",
            " 9   Extracurricular    2392 non-null   int64  \n",
            " 10  Sports             2392 non-null   int64  \n",
            " 11  Music              2392 non-null   int64  \n",
            " 12  Volunteering       2392 non-null   int64  \n",
            " 13  GPA                2392 non-null   float64\n",
            " 14  GradeClass         2392 non-null   float64\n",
            "dtypes: float64(3), int64(12)\n",
            "memory usage: 280.4 KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BmITSNIvsNB"
      },
      "source": [
        "### 4. Remove the columns not needed for Student performance prediction\n",
        "\n",
        "- Choose only the columns you consider to be valuable for your model training.\n",
        "- For example, StudentID might not be a good feature for your model, and thus should be removed from your main dataset, which other columns should also be removed?\n",
        "- You can name that final dataset as 'dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZrROF0UKvsNB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "3c029cd9-62d4-46ae-827e-7347ea071b23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Age  Gender  ParentalEducation  StudyTimeWeekly  Absences  Tutoring  \\\n",
              "0      17       1                  2        19.833723         7         1   \n",
              "1      18       0                  1        15.408756         0         0   \n",
              "2      15       0                  3         4.210570        26         0   \n",
              "3      17       1                  3        10.028829        14         0   \n",
              "4      17       1                  2         4.672495        17         1   \n",
              "...   ...     ...                ...              ...       ...       ...   \n",
              "2387   18       1                  3        10.680555         2         0   \n",
              "2388   17       0                  1         7.583217         4         1   \n",
              "2389   16       1                  2         6.805500        20         0   \n",
              "2390   16       1                  0        12.416653        17         0   \n",
              "2391   16       1                  2        17.819907        13         0   \n",
              "\n",
              "      ParentalSupport       GPA  Activities  \n",
              "0                   2  2.929196           1  \n",
              "1                   1  3.042915           0  \n",
              "2                   2  0.112602           0  \n",
              "3                   3  2.054218           1  \n",
              "4                   3  1.288061           0  \n",
              "...               ...       ...         ...  \n",
              "2387                4  3.455509           1  \n",
              "2388                4  3.279150           1  \n",
              "2389                2  1.142333           1  \n",
              "2390                2  1.803297           2  \n",
              "2391                2  2.140014           1  \n",
              "\n",
              "[2392 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85497e32-af68-46db-9cbc-2374402235d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>ParentalEducation</th>\n",
              "      <th>StudyTimeWeekly</th>\n",
              "      <th>Absences</th>\n",
              "      <th>Tutoring</th>\n",
              "      <th>ParentalSupport</th>\n",
              "      <th>GPA</th>\n",
              "      <th>Activities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>19.833723</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.929196</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15.408756</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.042915</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4.210570</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.112602</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>10.028829</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.054218</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4.672495</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.288061</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2387</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>10.680555</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3.455509</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2388</th>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.583217</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3.279150</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6.805500</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.142333</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2390</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12.416653</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.803297</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2391</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>17.819907</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2.140014</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2392 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85497e32-af68-46db-9cbc-2374402235d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-85497e32-af68-46db-9cbc-2374402235d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-85497e32-af68-46db-9cbc-2374402235d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-32edbc76-60a2-40b3-a8fc-83e548ea83d0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-32edbc76-60a2-40b3-a8fc-83e548ea83d0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-32edbc76-60a2-40b3-a8fc-83e548ea83d0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_23bc8240-52b6-4bae-b638-b56b09b273f4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_23bc8240-52b6-4bae-b638-b56b09b273f4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataset');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 2392,\n  \"fields\": [\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 15,\n        \"max\": 18,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          18,\n          16,\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ParentalEducation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"StudyTimeWeekly\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.65277423586027,\n        \"min\": 0.001056538645936,\n        \"max\": 19.97809399526153,\n        \"num_unique_values\": 2392,\n        \"samples\": [\n          0.1357634804717955,\n          1.9899245236127627\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Absences\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 29,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          18,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tutoring\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ParentalSupport\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GPA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.91515582032496,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 2371,\n        \"samples\": [\n          3.3104012689001965,\n          3.4577117259752934\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Activities\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Your code here\n",
        "dataset = data.drop(columns=['StudentID', 'GradeClass', 'Ethnicity'])\n",
        "dataset['Activities'] = dataset['Extracurricular'] + dataset['Sports'] + dataset['Music'] + dataset['Volunteering']\n",
        "dataset = dataset.drop(columns=['Extracurricular', 'Sports', 'Music', 'Volunteering'])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v53GaxyJvsNC"
      },
      "source": [
        "### 5. Check if the columns has any null values:\n",
        "- Here you now have your final dataset to use in your model training.\n",
        "- Before moving foward review your data check for any null or empty value that might be needed to be removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zgWk-bBEvsNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e12f57-7d64-4cf1-8874-7c59938c57af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "null_values = dataset.isnull().sum()\n",
        "print(null_values[null_values > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XaTAqWSvsNC"
      },
      "source": [
        "### 6. Prepare your data for training and for testing set:\n",
        " - First create a dataset named X, with all columns but GPA. These are the features\n",
        " - Next create another dataset named y, with only GPA column. This is the label\n",
        " - If you go to your Imports, you will see the following import: **'from sklearn.model_selection import train_test_split'**\n",
        " - Use that *train_test_split* function to create: X_train, X_test, y_train and y_test respectively. Use X and y datasets as parameters. Other parameters to use are: Test Size = 0.2, Random State = 42.\n",
        "\n",
        " - Standarize your features (X_train and X_test) by using the StandardScaler (investigate how to use fit_transform and transform functions). This will help the training process by dealing with normilized data.\n",
        "\n",
        " Note: Your X_train shape should be around (1913, 10). This means the dataset has 10 columns which should be the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nmJSx7OKvsNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dfd36eb-e1f2-40e6-a5a8-cd31e3fb8283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (1913, 8)\n",
            "X_train standardized shape: (1913, 8)\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "X = dataset.drop(columns=['GPA'])\n",
        "y = dataset['GPA']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "print(f\"X_train standardized shape: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHPYoFZlvsNC"
      },
      "source": [
        "### 7. Define your Deep Neural Network.\n",
        "- This will be a Sequential Neural Network.\n",
        "- With a Dense input layer with 64 units, and input dimention of 10 and Relu as the activation function.\n",
        "- A Dense hidden layer with 32 units, and Relu as the activation function.\n",
        "- And a Dense output layer with 1 unit, do not define an activation function so it defaults to linear, suitable for regression tasks. e.g. Dense(1)\n",
        "\n",
        "This last part of the output layer is super important, since we want to predict the GPA, this means that we want a regression and not a classification. Linear activation function is best for regression and Sigmoid is best for Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "h1NtwfWNvsNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "207a4323-ea56-48b3-a9ef-128007a0ebff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m576\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,689\u001b[0m (10.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689</span> (10.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,689\u001b[0m (10.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689</span> (10.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Your code here\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQDlmQOcvsNC"
      },
      "source": [
        "### 8. Compile your Neural Network\n",
        "- Choose Adam as the optimizer\n",
        "- And MSE as the Loss function\n",
        "- Also add the following metrics: Mean Absolute Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kkIWDBSRvsNC"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=['mae'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_ixNhQ3vsNC"
      },
      "source": [
        "### 9. Fit (or train) your model\n",
        "- Use the X_train and y_train datasets for the training\n",
        "- Do 50 data iterations\n",
        "- Choose the batch size = 10\n",
        "- Also select a validation_split of 0.2\n",
        "- Save the result of the fit function in a variable called 'history'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "paJ3Vds-vsND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5de84fb-f440-4b35-9f53-2d2103b57159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.2446 - mae: 1.1407 - val_loss: 0.1419 - val_mae: 0.3024\n",
            "Epoch 2/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1328 - mae: 0.2927 - val_loss: 0.0839 - val_mae: 0.2352\n",
            "Epoch 3/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0895 - mae: 0.2398 - val_loss: 0.0687 - val_mae: 0.2148\n",
            "Epoch 4/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0641 - mae: 0.2047 - val_loss: 0.0588 - val_mae: 0.1951\n",
            "Epoch 5/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0563 - mae: 0.1922 - val_loss: 0.0544 - val_mae: 0.1837\n",
            "Epoch 6/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0531 - mae: 0.1876 - val_loss: 0.0535 - val_mae: 0.1859\n",
            "Epoch 7/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0474 - mae: 0.1742 - val_loss: 0.0486 - val_mae: 0.1775\n",
            "Epoch 8/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0444 - mae: 0.1687 - val_loss: 0.0538 - val_mae: 0.1880\n",
            "Epoch 9/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0456 - mae: 0.1726 - val_loss: 0.0523 - val_mae: 0.1830\n",
            "Epoch 10/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0409 - mae: 0.1648 - val_loss: 0.0504 - val_mae: 0.1838\n",
            "Epoch 11/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0450 - mae: 0.1704 - val_loss: 0.0508 - val_mae: 0.1818\n",
            "Epoch 12/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0388 - mae: 0.1590 - val_loss: 0.0475 - val_mae: 0.1756\n",
            "Epoch 13/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0395 - mae: 0.1601 - val_loss: 0.0502 - val_mae: 0.1817\n",
            "Epoch 14/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0384 - mae: 0.1580 - val_loss: 0.0492 - val_mae: 0.1799\n",
            "Epoch 15/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0395 - mae: 0.1611 - val_loss: 0.0482 - val_mae: 0.1774\n",
            "Epoch 16/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0365 - mae: 0.1544 - val_loss: 0.0493 - val_mae: 0.1806\n",
            "Epoch 17/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0355 - mae: 0.1504 - val_loss: 0.0495 - val_mae: 0.1811\n",
            "Epoch 18/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0353 - mae: 0.1474 - val_loss: 0.0521 - val_mae: 0.1858\n",
            "Epoch 19/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0371 - mae: 0.1523 - val_loss: 0.0494 - val_mae: 0.1786\n",
            "Epoch 20/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0330 - mae: 0.1459 - val_loss: 0.0496 - val_mae: 0.1802\n",
            "Epoch 21/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0340 - mae: 0.1469 - val_loss: 0.0496 - val_mae: 0.1812\n",
            "Epoch 22/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0355 - mae: 0.1517 - val_loss: 0.0553 - val_mae: 0.1890\n",
            "Epoch 23/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0371 - mae: 0.1537 - val_loss: 0.0525 - val_mae: 0.1870\n",
            "Epoch 24/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0358 - mae: 0.1493 - val_loss: 0.0543 - val_mae: 0.1877\n",
            "Epoch 25/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0344 - mae: 0.1487 - val_loss: 0.0520 - val_mae: 0.1871\n",
            "Epoch 26/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0331 - mae: 0.1450 - val_loss: 0.0536 - val_mae: 0.1868\n",
            "Epoch 27/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0339 - mae: 0.1472 - val_loss: 0.0547 - val_mae: 0.1879\n",
            "Epoch 28/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0357 - mae: 0.1512 - val_loss: 0.0514 - val_mae: 0.1836\n",
            "Epoch 29/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0307 - mae: 0.1396 - val_loss: 0.0504 - val_mae: 0.1820\n",
            "Epoch 30/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0339 - mae: 0.1482 - val_loss: 0.0519 - val_mae: 0.1848\n",
            "Epoch 31/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0303 - mae: 0.1384 - val_loss: 0.0512 - val_mae: 0.1849\n",
            "Epoch 32/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0323 - mae: 0.1460 - val_loss: 0.0508 - val_mae: 0.1814\n",
            "Epoch 33/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0305 - mae: 0.1389 - val_loss: 0.0542 - val_mae: 0.1866\n",
            "Epoch 34/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0298 - mae: 0.1366 - val_loss: 0.0549 - val_mae: 0.1853\n",
            "Epoch 35/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0506 - val_mae: 0.1801\n",
            "Epoch 36/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0333 - mae: 0.1463 - val_loss: 0.0513 - val_mae: 0.1830\n",
            "Epoch 37/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0326 - mae: 0.1434 - val_loss: 0.0591 - val_mae: 0.1974\n",
            "Epoch 38/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0298 - mae: 0.1375 - val_loss: 0.0560 - val_mae: 0.1916\n",
            "Epoch 39/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0292 - mae: 0.1372 - val_loss: 0.0524 - val_mae: 0.1830\n",
            "Epoch 40/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0328 - mae: 0.1457 - val_loss: 0.0557 - val_mae: 0.1906\n",
            "Epoch 41/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0277 - mae: 0.1317 - val_loss: 0.0608 - val_mae: 0.1981\n",
            "Epoch 42/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0297 - mae: 0.1368 - val_loss: 0.0528 - val_mae: 0.1888\n",
            "Epoch 43/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0284 - mae: 0.1356 - val_loss: 0.0542 - val_mae: 0.1901\n",
            "Epoch 44/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0269 - mae: 0.1314 - val_loss: 0.0538 - val_mae: 0.1867\n",
            "Epoch 45/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0269 - mae: 0.1302 - val_loss: 0.0545 - val_mae: 0.1866\n",
            "Epoch 46/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0275 - mae: 0.1330 - val_loss: 0.0568 - val_mae: 0.1932\n",
            "Epoch 47/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0292 - mae: 0.1377 - val_loss: 0.0590 - val_mae: 0.1942\n",
            "Epoch 48/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0274 - mae: 0.1319 - val_loss: 0.0592 - val_mae: 0.1977\n",
            "Epoch 49/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0261 - mae: 0.1287 - val_loss: 0.0550 - val_mae: 0.1883\n",
            "Epoch 50/50\n",
            "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0266 - mae: 0.1304 - val_loss: 0.0536 - val_mae: 0.1867\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afJlAkSPvsND"
      },
      "source": [
        "### 10. View your history variable:\n",
        "- Use Matplotlib.pyplot to show graphs of your model traning history\n",
        "- In one graph:\n",
        "   - Plot the Training Loss and the Validation Loss\n",
        "   - X Label = Epochs\n",
        "   - Y Label = Loss\n",
        "   - Title = Training and Validation Loss over Epochs\n",
        "- In a second graph:\n",
        "   - Plot the Training MAE and the Validation MAE\n",
        "   - X Label = Epochs\n",
        "   - Y Label = Mean Absolute Error (MAE)\n",
        "   - Title = Training and Validation MAE over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5I9gck1tvsND",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "8c577f83-a525-448e-bd39-c9064f5cd513"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Your code here\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Primer grafico:\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Segundo grafico:\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(mae, label='Training MAE')\n",
        "plt.plot(val_mae, label='Validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Mean Absolute Error (MAE)')\n",
        "plt.title('Training and Validation MAE over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ubfY_8JvsND"
      },
      "source": [
        "### 11. Evaluate your model:\n",
        "- See the result of your loss function.\n",
        "- What can you deduct from there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "yOU6YtOPvsND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de419d40-5bdb-4d0a-a3e0-8d1d72cbb58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0641 - mae: 0.1984\n",
            "Test Loss (MSE): 0.061365775763988495\n",
            "Test Mean Absolute Error (MAE): 0.19605964422225952\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Test Loss (MSE): {test_loss}\")\n",
        "print(f\"Test Mean Absolute Error (MAE): {test_mae}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsoedtY8vsND"
      },
      "source": [
        "### 12. Use your model to make some predictions:\n",
        "- Make predictions of your X_test dataset\n",
        "- Print the each of the predictions and the actual value (which is in y_test)\n",
        "- How good was your model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "b1wn1PGkvsND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1a0e93-1cb2-4c23-d93c-81e1d334fe3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "Predicted GPA: 1.5209, Actual GPA: 1.4277\n",
            "Predicted GPA: 2.9702, Actual GPA: 3.1174\n",
            "Predicted GPA: 1.8314, Actual GPA: 2.0378\n",
            "Predicted GPA: 3.3882, Actual GPA: 3.5485\n",
            "Predicted GPA: 0.3961, Actual GPA: 0.2490\n",
            "Predicted GPA: 2.7210, Actual GPA: 2.6277\n",
            "Predicted GPA: 1.4767, Actual GPA: 2.0574\n",
            "Predicted GPA: 2.2470, Actual GPA: 2.2483\n",
            "Predicted GPA: 2.1711, Actual GPA: 2.1947\n",
            "Predicted GPA: 1.0110, Actual GPA: 0.7582\n",
            "Predicted GPA: 2.4943, Actual GPA: 2.3709\n",
            "Predicted GPA: 0.6176, Actual GPA: 0.7664\n",
            "Predicted GPA: 2.8382, Actual GPA: 2.9527\n",
            "Predicted GPA: 2.7169, Actual GPA: 2.3433\n",
            "Predicted GPA: 2.8507, Actual GPA: 2.7718\n",
            "Predicted GPA: 0.4930, Actual GPA: 0.2879\n",
            "Predicted GPA: 1.0995, Actual GPA: 1.0183\n",
            "Predicted GPA: 1.3203, Actual GPA: 1.6294\n",
            "Predicted GPA: 2.0747, Actual GPA: 2.0744\n",
            "Predicted GPA: 2.8458, Actual GPA: 2.4238\n",
            "Predicted GPA: 2.1809, Actual GPA: 1.7562\n",
            "Predicted GPA: 1.7963, Actual GPA: 1.5663\n",
            "Predicted GPA: 1.7298, Actual GPA: 1.7062\n",
            "Predicted GPA: 3.1443, Actual GPA: 3.1614\n",
            "Predicted GPA: 1.7880, Actual GPA: 1.7334\n",
            "Predicted GPA: 0.6275, Actual GPA: 0.8420\n",
            "Predicted GPA: 1.8851, Actual GPA: 1.3792\n",
            "Predicted GPA: 2.5256, Actual GPA: 3.0270\n",
            "Predicted GPA: 1.9928, Actual GPA: 2.1920\n",
            "Predicted GPA: 1.7152, Actual GPA: 2.3158\n",
            "Predicted GPA: 2.0388, Actual GPA: 2.0681\n",
            "Predicted GPA: 0.7147, Actual GPA: 0.8691\n",
            "Predicted GPA: 2.9401, Actual GPA: 2.9001\n",
            "Predicted GPA: 3.0742, Actual GPA: 3.4686\n",
            "Predicted GPA: 1.3506, Actual GPA: 1.5674\n",
            "Predicted GPA: 2.0166, Actual GPA: 1.7947\n",
            "Predicted GPA: 2.9665, Actual GPA: 3.1813\n",
            "Predicted GPA: 2.9776, Actual GPA: 2.8974\n",
            "Predicted GPA: 3.2049, Actual GPA: 3.2449\n",
            "Predicted GPA: 0.8985, Actual GPA: 0.3578\n",
            "Predicted GPA: 2.5999, Actual GPA: 2.6524\n",
            "Predicted GPA: 3.3224, Actual GPA: 3.6810\n",
            "Predicted GPA: 1.4177, Actual GPA: 1.0364\n",
            "Predicted GPA: 2.3232, Actual GPA: 2.0172\n",
            "Predicted GPA: 0.9484, Actual GPA: 0.9634\n",
            "Predicted GPA: 2.8644, Actual GPA: 2.2395\n",
            "Predicted GPA: 2.9934, Actual GPA: 2.7360\n",
            "Predicted GPA: 1.1033, Actual GPA: 1.3619\n",
            "Predicted GPA: 2.6677, Actual GPA: 2.7030\n",
            "Predicted GPA: 1.5077, Actual GPA: 1.4419\n",
            "Predicted GPA: 3.1673, Actual GPA: 3.2195\n",
            "Predicted GPA: 3.0876, Actual GPA: 3.3391\n",
            "Predicted GPA: 1.7516, Actual GPA: 1.5562\n",
            "Predicted GPA: 1.2778, Actual GPA: 1.3424\n",
            "Predicted GPA: 1.8985, Actual GPA: 1.7562\n",
            "Predicted GPA: 3.4740, Actual GPA: 3.4218\n",
            "Predicted GPA: 2.5981, Actual GPA: 2.3170\n",
            "Predicted GPA: 3.3234, Actual GPA: 3.2866\n",
            "Predicted GPA: 1.1171, Actual GPA: 0.6847\n",
            "Predicted GPA: 2.2126, Actual GPA: 2.1371\n",
            "Predicted GPA: 1.8792, Actual GPA: 1.6885\n",
            "Predicted GPA: 1.7539, Actual GPA: 2.1962\n",
            "Predicted GPA: 2.3533, Actual GPA: 2.4778\n",
            "Predicted GPA: 1.5103, Actual GPA: 1.1979\n",
            "Predicted GPA: 1.1627, Actual GPA: 0.9882\n",
            "Predicted GPA: 2.2575, Actual GPA: 1.8589\n",
            "Predicted GPA: 3.4314, Actual GPA: 3.0407\n",
            "Predicted GPA: 2.7696, Actual GPA: 2.3741\n",
            "Predicted GPA: 1.3069, Actual GPA: 1.2213\n",
            "Predicted GPA: 3.6827, Actual GPA: 3.2742\n",
            "Predicted GPA: 3.3757, Actual GPA: 3.5452\n",
            "Predicted GPA: 0.8875, Actual GPA: 1.1019\n",
            "Predicted GPA: 3.0778, Actual GPA: 2.9688\n",
            "Predicted GPA: 2.7026, Actual GPA: 2.5762\n",
            "Predicted GPA: 0.9169, Actual GPA: 0.4541\n",
            "Predicted GPA: 2.9572, Actual GPA: 2.6742\n",
            "Predicted GPA: 2.0266, Actual GPA: 2.0600\n",
            "Predicted GPA: 1.8380, Actual GPA: 1.9831\n",
            "Predicted GPA: 0.3232, Actual GPA: 0.2124\n",
            "Predicted GPA: 1.4896, Actual GPA: 1.2794\n",
            "Predicted GPA: 1.8512, Actual GPA: 2.0297\n",
            "Predicted GPA: 3.0754, Actual GPA: 3.1892\n",
            "Predicted GPA: 2.1451, Actual GPA: 2.2565\n",
            "Predicted GPA: 1.6383, Actual GPA: 1.6364\n",
            "Predicted GPA: 1.4722, Actual GPA: 1.4016\n",
            "Predicted GPA: 0.4881, Actual GPA: 0.4436\n",
            "Predicted GPA: 3.9946, Actual GPA: 4.0000\n",
            "Predicted GPA: 1.2568, Actual GPA: 1.2712\n",
            "Predicted GPA: 1.4039, Actual GPA: 1.3026\n",
            "Predicted GPA: 1.7888, Actual GPA: 1.5970\n",
            "Predicted GPA: 2.6995, Actual GPA: 2.5354\n",
            "Predicted GPA: 1.5966, Actual GPA: 1.3829\n",
            "Predicted GPA: 1.0235, Actual GPA: 1.0385\n",
            "Predicted GPA: 0.7867, Actual GPA: 0.1307\n",
            "Predicted GPA: 1.5139, Actual GPA: 1.2738\n",
            "Predicted GPA: 1.1069, Actual GPA: 1.2715\n",
            "Predicted GPA: 3.5937, Actual GPA: 3.3251\n",
            "Predicted GPA: 1.5131, Actual GPA: 1.0707\n",
            "Predicted GPA: 1.5539, Actual GPA: 1.5743\n",
            "Predicted GPA: 1.9102, Actual GPA: 1.6337\n",
            "Predicted GPA: 0.3559, Actual GPA: 0.0000\n",
            "Predicted GPA: 2.3050, Actual GPA: 2.1221\n",
            "Predicted GPA: 1.1295, Actual GPA: 1.2912\n",
            "Predicted GPA: 1.6069, Actual GPA: 1.4547\n",
            "Predicted GPA: 1.0065, Actual GPA: 0.7743\n",
            "Predicted GPA: 2.1088, Actual GPA: 2.2931\n",
            "Predicted GPA: 2.6914, Actual GPA: 2.6828\n",
            "Predicted GPA: 1.3217, Actual GPA: 1.3155\n",
            "Predicted GPA: 1.5281, Actual GPA: 1.4694\n",
            "Predicted GPA: 0.6118, Actual GPA: 0.6132\n",
            "Predicted GPA: 1.6145, Actual GPA: 1.8820\n",
            "Predicted GPA: 2.0659, Actual GPA: 2.2689\n",
            "Predicted GPA: 3.3095, Actual GPA: 3.5452\n",
            "Predicted GPA: 0.4383, Actual GPA: 0.4258\n",
            "Predicted GPA: 0.4358, Actual GPA: 0.5404\n",
            "Predicted GPA: 1.3083, Actual GPA: 1.1111\n",
            "Predicted GPA: 2.1854, Actual GPA: 2.0354\n",
            "Predicted GPA: 2.9499, Actual GPA: 2.9955\n",
            "Predicted GPA: 0.6240, Actual GPA: 1.2179\n",
            "Predicted GPA: 1.8151, Actual GPA: 2.1270\n",
            "Predicted GPA: 0.8811, Actual GPA: 0.5298\n",
            "Predicted GPA: 3.0266, Actual GPA: 3.0169\n",
            "Predicted GPA: 3.6481, Actual GPA: 3.5921\n",
            "Predicted GPA: 0.5883, Actual GPA: 1.0583\n",
            "Predicted GPA: 1.6303, Actual GPA: 1.8426\n",
            "Predicted GPA: 1.1145, Actual GPA: 1.2546\n",
            "Predicted GPA: 2.6195, Actual GPA: 2.7949\n",
            "Predicted GPA: 0.9210, Actual GPA: 1.3479\n",
            "Predicted GPA: 2.2206, Actual GPA: 2.2270\n",
            "Predicted GPA: 2.7343, Actual GPA: 2.8135\n",
            "Predicted GPA: 1.1049, Actual GPA: 1.1056\n",
            "Predicted GPA: 4.2408, Actual GPA: 4.0000\n",
            "Predicted GPA: 2.9460, Actual GPA: 2.8982\n",
            "Predicted GPA: 0.4263, Actual GPA: 0.4240\n",
            "Predicted GPA: 0.5941, Actual GPA: 0.6915\n",
            "Predicted GPA: 0.7581, Actual GPA: 0.9300\n",
            "Predicted GPA: 2.0520, Actual GPA: 2.3308\n",
            "Predicted GPA: 0.2594, Actual GPA: 0.3376\n",
            "Predicted GPA: 0.2675, Actual GPA: 0.5022\n",
            "Predicted GPA: 1.2100, Actual GPA: 0.9893\n",
            "Predicted GPA: 2.6919, Actual GPA: 2.7459\n",
            "Predicted GPA: 1.6489, Actual GPA: 1.8042\n",
            "Predicted GPA: 3.2379, Actual GPA: 3.3433\n",
            "Predicted GPA: 1.9335, Actual GPA: 1.5731\n",
            "Predicted GPA: 2.9303, Actual GPA: 2.9640\n",
            "Predicted GPA: 1.3318, Actual GPA: 1.1936\n",
            "Predicted GPA: 1.4385, Actual GPA: 1.3493\n",
            "Predicted GPA: 2.1316, Actual GPA: 2.5202\n",
            "Predicted GPA: 2.9685, Actual GPA: 3.2538\n",
            "Predicted GPA: 1.2442, Actual GPA: 1.4634\n",
            "Predicted GPA: 3.2521, Actual GPA: 3.1620\n",
            "Predicted GPA: 0.9103, Actual GPA: 0.7696\n",
            "Predicted GPA: 3.3387, Actual GPA: 3.5037\n",
            "Predicted GPA: 1.7861, Actual GPA: 1.2688\n",
            "Predicted GPA: 1.0977, Actual GPA: 1.1708\n",
            "Predicted GPA: 0.5378, Actual GPA: 0.4713\n",
            "Predicted GPA: 1.4469, Actual GPA: 1.3979\n",
            "Predicted GPA: 0.5345, Actual GPA: 0.9690\n",
            "Predicted GPA: 1.2895, Actual GPA: 1.5113\n",
            "Predicted GPA: 0.6545, Actual GPA: 0.5242\n",
            "Predicted GPA: 1.5210, Actual GPA: 1.4914\n",
            "Predicted GPA: 1.0724, Actual GPA: 1.5194\n",
            "Predicted GPA: 0.9353, Actual GPA: 0.7133\n",
            "Predicted GPA: 0.8041, Actual GPA: 0.4006\n",
            "Predicted GPA: 0.1542, Actual GPA: 0.5814\n",
            "Predicted GPA: 0.7434, Actual GPA: 0.4051\n",
            "Predicted GPA: 0.8125, Actual GPA: 1.1544\n",
            "Predicted GPA: 1.3471, Actual GPA: 1.1502\n",
            "Predicted GPA: 2.4555, Actual GPA: 2.4579\n",
            "Predicted GPA: 3.1708, Actual GPA: 3.4555\n",
            "Predicted GPA: 1.6341, Actual GPA: 1.4414\n",
            "Predicted GPA: 3.3295, Actual GPA: 3.2880\n",
            "Predicted GPA: 2.9028, Actual GPA: 3.0439\n",
            "Predicted GPA: 1.2874, Actual GPA: 0.1898\n",
            "Predicted GPA: 1.3284, Actual GPA: 1.1658\n",
            "Predicted GPA: 2.4768, Actual GPA: 2.5654\n",
            "Predicted GPA: 2.4920, Actual GPA: 2.2347\n",
            "Predicted GPA: 1.2771, Actual GPA: 1.2587\n",
            "Predicted GPA: 2.1769, Actual GPA: 2.1167\n",
            "Predicted GPA: 1.5561, Actual GPA: 1.6747\n",
            "Predicted GPA: 3.0180, Actual GPA: 2.8211\n",
            "Predicted GPA: 1.6723, Actual GPA: 1.9961\n",
            "Predicted GPA: 0.6377, Actual GPA: 0.8091\n",
            "Predicted GPA: 3.2050, Actual GPA: 2.5466\n",
            "Predicted GPA: 2.9490, Actual GPA: 2.7350\n",
            "Predicted GPA: 3.0476, Actual GPA: 2.7536\n",
            "Predicted GPA: 3.3109, Actual GPA: 3.2839\n",
            "Predicted GPA: 1.3834, Actual GPA: 1.2878\n",
            "Predicted GPA: 0.4366, Actual GPA: 0.3107\n",
            "Predicted GPA: 2.0138, Actual GPA: 1.8315\n",
            "Predicted GPA: 1.1141, Actual GPA: 0.9116\n",
            "Predicted GPA: 2.1535, Actual GPA: 2.1097\n",
            "Predicted GPA: 1.2804, Actual GPA: 1.0445\n",
            "Predicted GPA: 0.8163, Actual GPA: 0.9655\n",
            "Predicted GPA: 1.8842, Actual GPA: 1.5504\n",
            "Predicted GPA: 1.5188, Actual GPA: 1.2238\n",
            "Predicted GPA: 2.0194, Actual GPA: 1.8934\n",
            "Predicted GPA: 1.9677, Actual GPA: 2.2954\n",
            "Predicted GPA: 2.7624, Actual GPA: 2.8393\n",
            "Predicted GPA: 2.4724, Actual GPA: 2.2518\n",
            "Predicted GPA: 1.5803, Actual GPA: 1.4053\n",
            "Predicted GPA: 2.9998, Actual GPA: 3.0321\n",
            "Predicted GPA: 0.7535, Actual GPA: 0.5694\n",
            "Predicted GPA: 2.1719, Actual GPA: 2.2201\n",
            "Predicted GPA: 3.0469, Actual GPA: 2.7374\n",
            "Predicted GPA: 2.0781, Actual GPA: 2.3408\n",
            "Predicted GPA: 0.3447, Actual GPA: 0.5602\n",
            "Predicted GPA: 3.2525, Actual GPA: 2.9139\n",
            "Predicted GPA: 2.2370, Actual GPA: 2.2595\n",
            "Predicted GPA: 1.5769, Actual GPA: 1.7420\n",
            "Predicted GPA: 1.6682, Actual GPA: 1.2659\n",
            "Predicted GPA: 0.9886, Actual GPA: 1.2987\n",
            "Predicted GPA: 2.3449, Actual GPA: 1.9138\n",
            "Predicted GPA: 2.7242, Actual GPA: 2.7808\n",
            "Predicted GPA: 2.3743, Actual GPA: 2.5535\n",
            "Predicted GPA: 0.5360, Actual GPA: 0.8923\n",
            "Predicted GPA: 2.9542, Actual GPA: 2.6697\n",
            "Predicted GPA: 3.2494, Actual GPA: 3.0885\n",
            "Predicted GPA: 1.7620, Actual GPA: 1.5636\n",
            "Predicted GPA: 2.9224, Actual GPA: 2.8757\n",
            "Predicted GPA: 1.1707, Actual GPA: 1.1845\n",
            "Predicted GPA: 2.4844, Actual GPA: 2.5842\n",
            "Predicted GPA: 3.3610, Actual GPA: 3.8128\n",
            "Predicted GPA: 0.5692, Actual GPA: 0.6729\n",
            "Predicted GPA: 1.5071, Actual GPA: 1.9559\n",
            "Predicted GPA: 1.6361, Actual GPA: 1.7733\n",
            "Predicted GPA: 3.2108, Actual GPA: 3.0554\n",
            "Predicted GPA: 2.8488, Actual GPA: 2.7185\n",
            "Predicted GPA: 1.2695, Actual GPA: 1.2705\n",
            "Predicted GPA: 2.0421, Actual GPA: 1.9880\n",
            "Predicted GPA: 1.6543, Actual GPA: 1.7209\n",
            "Predicted GPA: 1.8857, Actual GPA: 2.1106\n",
            "Predicted GPA: 2.6721, Actual GPA: 2.8816\n",
            "Predicted GPA: 1.1054, Actual GPA: 0.9708\n",
            "Predicted GPA: 2.9012, Actual GPA: 2.9729\n",
            "Predicted GPA: 2.7318, Actual GPA: 2.5210\n",
            "Predicted GPA: 0.6700, Actual GPA: 0.5152\n",
            "Predicted GPA: 1.1090, Actual GPA: 1.0086\n",
            "Predicted GPA: 2.6877, Actual GPA: 2.7913\n",
            "Predicted GPA: 0.8066, Actual GPA: 0.5868\n",
            "Predicted GPA: 3.4369, Actual GPA: 3.6663\n",
            "Predicted GPA: 1.1734, Actual GPA: 0.9031\n",
            "Predicted GPA: 0.5617, Actual GPA: 0.7046\n",
            "Predicted GPA: 2.2262, Actual GPA: 2.3983\n",
            "Predicted GPA: 1.3688, Actual GPA: 1.8178\n",
            "Predicted GPA: 0.6947, Actual GPA: 0.7996\n",
            "Predicted GPA: 2.4121, Actual GPA: 2.1274\n",
            "Predicted GPA: 1.2291, Actual GPA: 1.1768\n",
            "Predicted GPA: 0.6118, Actual GPA: 0.4291\n",
            "Predicted GPA: 1.6883, Actual GPA: 2.0469\n",
            "Predicted GPA: 1.2036, Actual GPA: 1.0862\n",
            "Predicted GPA: 3.0936, Actual GPA: 2.9869\n",
            "Predicted GPA: 2.6070, Actual GPA: 2.4740\n",
            "Predicted GPA: 2.0265, Actual GPA: 1.5819\n",
            "Predicted GPA: 2.4091, Actual GPA: 2.3278\n",
            "Predicted GPA: 1.5589, Actual GPA: 1.5253\n",
            "Predicted GPA: 1.4772, Actual GPA: 1.6831\n",
            "Predicted GPA: 2.2912, Actual GPA: 2.4610\n",
            "Predicted GPA: 0.5218, Actual GPA: 0.0278\n",
            "Predicted GPA: 1.6126, Actual GPA: 1.5364\n",
            "Predicted GPA: 1.9470, Actual GPA: 2.0999\n",
            "Predicted GPA: 0.4668, Actual GPA: 0.3751\n",
            "Predicted GPA: 1.5235, Actual GPA: 1.2946\n",
            "Predicted GPA: 0.7981, Actual GPA: 0.7697\n",
            "Predicted GPA: 1.7016, Actual GPA: 1.8214\n",
            "Predicted GPA: 3.4790, Actual GPA: 3.4983\n",
            "Predicted GPA: 2.7630, Actual GPA: 2.3781\n",
            "Predicted GPA: 1.0258, Actual GPA: 0.5185\n",
            "Predicted GPA: 2.4987, Actual GPA: 2.7549\n",
            "Predicted GPA: 2.0210, Actual GPA: 2.2070\n",
            "Predicted GPA: 1.2834, Actual GPA: 1.1416\n",
            "Predicted GPA: 3.2456, Actual GPA: 3.1376\n",
            "Predicted GPA: 1.5410, Actual GPA: 1.3638\n",
            "Predicted GPA: 1.3757, Actual GPA: 1.6885\n",
            "Predicted GPA: 1.1193, Actual GPA: 1.3503\n",
            "Predicted GPA: 2.7182, Actual GPA: 2.0625\n",
            "Predicted GPA: 1.0986, Actual GPA: 1.0226\n",
            "Predicted GPA: 1.6932, Actual GPA: 1.7923\n",
            "Predicted GPA: 0.3440, Actual GPA: 0.0000\n",
            "Predicted GPA: 2.7043, Actual GPA: 2.8729\n",
            "Predicted GPA: 3.4043, Actual GPA: 3.0887\n",
            "Predicted GPA: 2.6855, Actual GPA: 2.9375\n",
            "Predicted GPA: 1.9320, Actual GPA: 1.9901\n",
            "Predicted GPA: 1.5465, Actual GPA: 1.5165\n",
            "Predicted GPA: 2.9326, Actual GPA: 3.3009\n",
            "Predicted GPA: 2.4909, Actual GPA: 2.1727\n",
            "Predicted GPA: 1.5733, Actual GPA: 1.1194\n",
            "Predicted GPA: 1.4446, Actual GPA: 1.5718\n",
            "Predicted GPA: 0.8049, Actual GPA: 0.4959\n",
            "Predicted GPA: 0.6483, Actual GPA: 0.1550\n",
            "Predicted GPA: 3.4549, Actual GPA: 3.8650\n",
            "Predicted GPA: 1.6739, Actual GPA: 1.8044\n",
            "Predicted GPA: 2.3786, Actual GPA: 2.7842\n",
            "Predicted GPA: 1.3445, Actual GPA: 1.2059\n",
            "Predicted GPA: 2.3859, Actual GPA: 2.8236\n",
            "Predicted GPA: 1.3027, Actual GPA: 0.8952\n",
            "Predicted GPA: 2.8654, Actual GPA: 3.1123\n",
            "Predicted GPA: 1.8007, Actual GPA: 1.8154\n",
            "Predicted GPA: 0.3812, Actual GPA: 0.1006\n",
            "Predicted GPA: 2.8641, Actual GPA: 3.0655\n",
            "Predicted GPA: 2.7693, Actual GPA: 2.5539\n",
            "Predicted GPA: 2.0674, Actual GPA: 1.9864\n",
            "Predicted GPA: 0.9681, Actual GPA: 0.9941\n",
            "Predicted GPA: 3.3089, Actual GPA: 3.3501\n",
            "Predicted GPA: 2.4326, Actual GPA: 2.3117\n",
            "Predicted GPA: 2.5300, Actual GPA: 2.5042\n",
            "Predicted GPA: 0.6200, Actual GPA: 0.7635\n",
            "Predicted GPA: 2.4828, Actual GPA: 2.7122\n",
            "Predicted GPA: 0.6938, Actual GPA: 0.9253\n",
            "Predicted GPA: 1.8754, Actual GPA: 1.8498\n",
            "Predicted GPA: 1.4185, Actual GPA: 1.3971\n",
            "Predicted GPA: 1.7170, Actual GPA: 1.8890\n",
            "Predicted GPA: 0.6392, Actual GPA: 0.9000\n",
            "Predicted GPA: 1.1003, Actual GPA: 1.0277\n",
            "Predicted GPA: 0.8528, Actual GPA: 0.7017\n",
            "Predicted GPA: 1.4509, Actual GPA: 1.4120\n",
            "Predicted GPA: 3.0270, Actual GPA: 2.9774\n",
            "Predicted GPA: 2.3645, Actual GPA: 2.2104\n",
            "Predicted GPA: 2.5973, Actual GPA: 2.4387\n",
            "Predicted GPA: 1.8858, Actual GPA: 1.9213\n",
            "Predicted GPA: 2.0206, Actual GPA: 1.7340\n",
            "Predicted GPA: 1.1996, Actual GPA: 1.5893\n",
            "Predicted GPA: 2.0227, Actual GPA: 1.5034\n",
            "Predicted GPA: 2.2574, Actual GPA: 2.2220\n",
            "Predicted GPA: 3.1453, Actual GPA: 3.1062\n",
            "Predicted GPA: 1.1710, Actual GPA: 1.5421\n",
            "Predicted GPA: 1.0146, Actual GPA: 1.2523\n",
            "Predicted GPA: 1.5879, Actual GPA: 1.7105\n",
            "Predicted GPA: 2.7915, Actual GPA: 2.6936\n",
            "Predicted GPA: 1.1208, Actual GPA: 0.9657\n",
            "Predicted GPA: 1.9343, Actual GPA: 1.8683\n",
            "Predicted GPA: 2.9540, Actual GPA: 2.8069\n",
            "Predicted GPA: 2.2646, Actual GPA: 2.3428\n",
            "Predicted GPA: 2.5057, Actual GPA: 2.5837\n",
            "Predicted GPA: 3.2963, Actual GPA: 3.2486\n",
            "Predicted GPA: 2.5587, Actual GPA: 2.5845\n",
            "Predicted GPA: 2.6827, Actual GPA: 2.5876\n",
            "Predicted GPA: 1.3934, Actual GPA: 1.5660\n",
            "Predicted GPA: 0.6141, Actual GPA: 0.8670\n",
            "Predicted GPA: 2.2252, Actual GPA: 1.8476\n",
            "Predicted GPA: 1.5231, Actual GPA: 1.1221\n",
            "Predicted GPA: 1.4379, Actual GPA: 1.7182\n",
            "Predicted GPA: 1.3855, Actual GPA: 1.1645\n",
            "Predicted GPA: 1.7105, Actual GPA: 1.6833\n",
            "Predicted GPA: 2.0954, Actual GPA: 2.0186\n",
            "Predicted GPA: 2.4629, Actual GPA: 2.5125\n",
            "Predicted GPA: 3.0138, Actual GPA: 3.1690\n",
            "Predicted GPA: 3.3858, Actual GPA: 3.0526\n",
            "Predicted GPA: 1.0161, Actual GPA: 1.1529\n",
            "Predicted GPA: 3.0752, Actual GPA: 3.2583\n",
            "Predicted GPA: 3.4958, Actual GPA: 3.3637\n",
            "Predicted GPA: 2.3228, Actual GPA: 2.4041\n",
            "Predicted GPA: 1.8904, Actual GPA: 2.1377\n",
            "Predicted GPA: 2.6085, Actual GPA: 2.4276\n",
            "Predicted GPA: 2.0168, Actual GPA: 1.8100\n",
            "Predicted GPA: 1.8538, Actual GPA: 1.7560\n",
            "Predicted GPA: 2.3832, Actual GPA: 2.0531\n",
            "Predicted GPA: 0.9760, Actual GPA: 1.6015\n",
            "Predicted GPA: 0.8076, Actual GPA: 0.4696\n",
            "Predicted GPA: 2.3863, Actual GPA: 2.0046\n",
            "Predicted GPA: 2.7386, Actual GPA: 2.2888\n",
            "Predicted GPA: 3.0949, Actual GPA: 3.0917\n",
            "Predicted GPA: 2.2615, Actual GPA: 2.1172\n",
            "Predicted GPA: 1.0372, Actual GPA: 1.3476\n",
            "Predicted GPA: 1.7517, Actual GPA: 1.5304\n",
            "Predicted GPA: 2.8421, Actual GPA: 2.7344\n",
            "Predicted GPA: 3.8766, Actual GPA: 3.8302\n",
            "Predicted GPA: 2.6604, Actual GPA: 2.6394\n",
            "Predicted GPA: 2.6261, Actual GPA: 2.4714\n",
            "Predicted GPA: 2.1514, Actual GPA: 2.2286\n",
            "Predicted GPA: 0.8187, Actual GPA: 0.6566\n",
            "Predicted GPA: 2.1831, Actual GPA: 2.2598\n",
            "Predicted GPA: 2.4256, Actual GPA: 2.3328\n",
            "Predicted GPA: 2.4456, Actual GPA: 3.0235\n",
            "Predicted GPA: 2.4234, Actual GPA: 2.9647\n",
            "Predicted GPA: 1.2708, Actual GPA: 0.9836\n",
            "Predicted GPA: 1.3564, Actual GPA: 1.4655\n",
            "Predicted GPA: 2.2491, Actual GPA: 2.2689\n",
            "Predicted GPA: 3.6232, Actual GPA: 3.5729\n",
            "Predicted GPA: 0.8633, Actual GPA: 0.6823\n",
            "Predicted GPA: 1.9993, Actual GPA: 1.9847\n",
            "Predicted GPA: 2.3720, Actual GPA: 2.5197\n",
            "Predicted GPA: 0.7861, Actual GPA: 0.5496\n",
            "Predicted GPA: 2.9015, Actual GPA: 2.9869\n",
            "Predicted GPA: 2.8241, Actual GPA: 2.6405\n",
            "Predicted GPA: 1.6231, Actual GPA: 1.4773\n",
            "Predicted GPA: 2.4380, Actual GPA: 1.9396\n",
            "Predicted GPA: 1.9900, Actual GPA: 1.8298\n",
            "Predicted GPA: 2.5563, Actual GPA: 2.6696\n",
            "Predicted GPA: 1.7844, Actual GPA: 1.5961\n",
            "Predicted GPA: 2.4372, Actual GPA: 2.2790\n",
            "Predicted GPA: 0.9635, Actual GPA: 1.1378\n",
            "Predicted GPA: 1.2043, Actual GPA: 0.6768\n",
            "Predicted GPA: 2.2299, Actual GPA: 2.5040\n",
            "Predicted GPA: 1.9690, Actual GPA: 2.1226\n",
            "Predicted GPA: 2.6744, Actual GPA: 2.9779\n",
            "Predicted GPA: 2.9963, Actual GPA: 2.8276\n",
            "Predicted GPA: 3.1376, Actual GPA: 2.9422\n",
            "Predicted GPA: 2.0810, Actual GPA: 2.3100\n",
            "Predicted GPA: 2.8806, Actual GPA: 2.7779\n",
            "Predicted GPA: 2.4753, Actual GPA: 2.5244\n",
            "Predicted GPA: 2.2262, Actual GPA: 2.1353\n",
            "Predicted GPA: 2.6683, Actual GPA: 2.2242\n",
            "Predicted GPA: 2.9757, Actual GPA: 3.2389\n",
            "Predicted GPA: 2.1519, Actual GPA: 2.2303\n",
            "Predicted GPA: 3.0721, Actual GPA: 3.0608\n",
            "Predicted GPA: 1.0389, Actual GPA: 1.3636\n",
            "Predicted GPA: 2.9517, Actual GPA: 3.6035\n",
            "Predicted GPA: 1.9445, Actual GPA: 2.1673\n",
            "Predicted GPA: 3.3543, Actual GPA: 3.3239\n",
            "Predicted GPA: 2.0392, Actual GPA: 2.0166\n",
            "Predicted GPA: 1.7937, Actual GPA: 1.3620\n",
            "Predicted GPA: 2.3211, Actual GPA: 2.1156\n",
            "Predicted GPA: 2.4063, Actual GPA: 2.4653\n",
            "Predicted GPA: 3.1715, Actual GPA: 3.0605\n",
            "Predicted GPA: 2.0704, Actual GPA: 1.9915\n",
            "Predicted GPA: 0.3027, Actual GPA: 0.2649\n",
            "Predicted GPA: 3.2733, Actual GPA: 3.6457\n",
            "Predicted GPA: 2.7487, Actual GPA: 2.5172\n",
            "Predicted GPA: 1.5199, Actual GPA: 1.5487\n",
            "Predicted GPA: 1.4038, Actual GPA: 1.5996\n",
            "Predicted GPA: 1.5254, Actual GPA: 1.5539\n",
            "Predicted GPA: 2.4689, Actual GPA: 2.5959\n",
            "Predicted GPA: 1.9400, Actual GPA: 1.8791\n",
            "Predicted GPA: 1.4245, Actual GPA: 1.5067\n",
            "Predicted GPA: 1.2179, Actual GPA: 1.1049\n",
            "Predicted GPA: 0.5336, Actual GPA: 0.3414\n",
            "Predicted GPA: 2.1488, Actual GPA: 2.4053\n",
            "Predicted GPA: 2.0922, Actual GPA: 1.6848\n",
            "Predicted GPA: 3.1892, Actual GPA: 2.9935\n",
            "Predicted GPA: 2.9283, Actual GPA: 3.4154\n",
            "Predicted GPA: 1.4584, Actual GPA: 1.7097\n",
            "Predicted GPA: 1.7747, Actual GPA: 1.5387\n",
            "Predicted GPA: 2.9074, Actual GPA: 2.9214\n",
            "Predicted GPA: 2.5953, Actual GPA: 2.6052\n",
            "Predicted GPA: 2.1563, Actual GPA: 2.4400\n",
            "Predicted GPA: 1.5381, Actual GPA: 1.4290\n",
            "Predicted GPA: 3.4714, Actual GPA: 3.3721\n",
            "Predicted GPA: 2.4477, Actual GPA: 2.1516\n",
            "Predicted GPA: 1.6179, Actual GPA: 1.5951\n",
            "Predicted GPA: 1.2324, Actual GPA: 1.4146\n",
            "Predicted GPA: 3.0633, Actual GPA: 3.1289\n",
            "Predicted GPA: 2.9621, Actual GPA: 2.5534\n",
            "Predicted GPA: 3.2581, Actual GPA: 3.1296\n",
            "Predicted GPA: 3.0807, Actual GPA: 2.9820\n",
            "Predicted GPA: 2.9598, Actual GPA: 2.9818\n",
            "Predicted GPA: 1.0221, Actual GPA: 1.2011\n",
            "Predicted GPA: 2.7496, Actual GPA: 2.7914\n",
            "Predicted GPA: 0.6764, Actual GPA: 0.1530\n",
            "Predicted GPA: 0.7723, Actual GPA: 0.6194\n",
            "Predicted GPA: 2.3603, Actual GPA: 2.2813\n",
            "Predicted GPA: 0.8077, Actual GPA: 0.4937\n",
            "Predicted GPA: 2.8840, Actual GPA: 2.9665\n",
            "Predicted GPA: 1.8896, Actual GPA: 1.9895\n",
            "Predicted GPA: 3.4033, Actual GPA: 3.5433\n",
            "Predicted GPA: 2.1065, Actual GPA: 1.9706\n",
            "Predicted GPA: 1.3003, Actual GPA: 1.3441\n",
            "Predicted GPA: 0.9953, Actual GPA: 1.0270\n",
            "Predicted GPA: 0.7255, Actual GPA: 0.4278\n",
            "Predicted GPA: 2.2524, Actual GPA: 1.9747\n",
            "Predicted GPA: 0.6783, Actual GPA: 1.2422\n",
            "Predicted GPA: 0.6483, Actual GPA: 0.3844\n",
            "Predicted GPA: 0.6211, Actual GPA: 0.4475\n",
            "Predicted GPA: 1.7127, Actual GPA: 1.7291\n",
            "Predicted GPA: 2.0445, Actual GPA: 2.1364\n",
            "Predicted GPA: 0.3776, Actual GPA: 0.2110\n",
            "Predicted GPA: 0.3545, Actual GPA: 0.3310\n",
            "Predicted GPA: 2.6181, Actual GPA: 2.8889\n",
            "Predicted GPA: 3.0607, Actual GPA: 3.2704\n",
            "Predicted GPA: 2.7366, Actual GPA: 2.5001\n",
            "Predicted GPA: 1.3701, Actual GPA: 1.2129\n",
            "Predicted GPA: 1.7508, Actual GPA: 2.0093\n",
            "Predicted GPA: 1.0533, Actual GPA: 1.5438\n",
            "Predicted GPA: 1.6335, Actual GPA: 1.4382\n",
            "Predicted GPA: 1.9187, Actual GPA: 1.5624\n",
            "Predicted GPA: 2.1692, Actual GPA: 2.1749\n",
            "Predicted GPA: 2.5877, Actual GPA: 2.3325\n",
            "Predicted GPA: 2.4123, Actual GPA: 2.7780\n",
            "Predicted GPA: 1.1081, Actual GPA: 0.8635\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "    print(f\"Predicted GPA: {predictions[i][0]:.4f}, Actual GPA: {y_test.iloc[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z85j9w6WvsND"
      },
      "source": [
        "### 13. Compete against this model:\n",
        "- Create two more different models to compete with this model\n",
        "- Here are a few ideas of things you can change:\n",
        "   - During Dataset data engineering:\n",
        "      - You can remove features that you think do not help in the training and prediction\n",
        "      - Feature Scaling: Ensure all features are on a similar scale (as you already did with StandardScaler)\n",
        "   - During Model Definition:\n",
        "      - You can change the Model Architecture (change the type or number of layers or the number of units)\n",
        "      - You can add dropout layers to prevent overfitting\n",
        "   - During Model Compile:\n",
        "      - You can try other optimizer when compiling your model, here some optimizer samples: Adam, RMSprop, or Adagrad.\n",
        "      - Try another Loss Function\n",
        "   - During Model Training:\n",
        "      - Encrease the number of Epochs\n",
        "      - Adjust the size of your batch\n",
        "- Explain in a Markdown cell which changes are you implementing\n",
        "- Show the comparison of your model versus the original model\n",
        "   \n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL7kLTmcvsND"
      },
      "source": [
        "#### Model 2:\n",
        "- Changes:\n",
        "   - Dataset Data Engineering\n",
        "   - Model Definition\n",
        "   - Model Compile\n",
        "   - Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4ZhcinOHvsND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477736a5-9050-46cc-83e3-9e0768b84aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 1.5985 - mae: 0.9909 - val_loss: 0.1454 - val_mae: 0.3118\n",
            "Epoch 2/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1993 - mae: 0.3558 - val_loss: 0.1048 - val_mae: 0.2674\n",
            "Epoch 3/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1498 - mae: 0.3127 - val_loss: 0.0891 - val_mae: 0.2477\n",
            "Epoch 4/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1234 - mae: 0.2773 - val_loss: 0.0761 - val_mae: 0.2225\n",
            "Epoch 5/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1167 - mae: 0.2717 - val_loss: 0.0645 - val_mae: 0.2079\n",
            "Epoch 6/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1007 - mae: 0.2566 - val_loss: 0.0565 - val_mae: 0.1938\n",
            "Epoch 7/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0939 - mae: 0.2436 - val_loss: 0.0636 - val_mae: 0.1983\n",
            "Epoch 8/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0910 - mae: 0.2387 - val_loss: 0.0539 - val_mae: 0.1865\n",
            "Epoch 9/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0782 - mae: 0.2250 - val_loss: 0.0504 - val_mae: 0.1858\n",
            "Epoch 10/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0764 - mae: 0.2208 - val_loss: 0.0565 - val_mae: 0.1875\n",
            "Epoch 11/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0684 - mae: 0.2107 - val_loss: 0.0553 - val_mae: 0.1916\n",
            "Epoch 12/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0672 - mae: 0.2091 - val_loss: 0.0458 - val_mae: 0.1736\n",
            "Epoch 13/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0689 - mae: 0.2085 - val_loss: 0.0490 - val_mae: 0.1814\n",
            "Epoch 14/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0628 - mae: 0.2007 - val_loss: 0.0509 - val_mae: 0.1838\n",
            "Epoch 15/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0661 - mae: 0.2050 - val_loss: 0.0449 - val_mae: 0.1731\n",
            "Epoch 16/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0616 - mae: 0.2014 - val_loss: 0.0468 - val_mae: 0.1769\n",
            "Epoch 17/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0625 - mae: 0.2022 - val_loss: 0.0601 - val_mae: 0.2004\n",
            "Epoch 18/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0650 - mae: 0.2018 - val_loss: 0.0491 - val_mae: 0.1784\n",
            "Epoch 19/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0572 - mae: 0.1919 - val_loss: 0.0444 - val_mae: 0.1721\n",
            "Epoch 20/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0578 - mae: 0.1907 - val_loss: 0.0474 - val_mae: 0.1761\n",
            "Epoch 21/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0596 - mae: 0.1942 - val_loss: 0.0441 - val_mae: 0.1693\n",
            "Epoch 22/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0560 - mae: 0.1874 - val_loss: 0.0447 - val_mae: 0.1707\n",
            "Epoch 23/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0541 - mae: 0.1868 - val_loss: 0.0476 - val_mae: 0.1804\n",
            "Epoch 24/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0555 - mae: 0.1888 - val_loss: 0.0484 - val_mae: 0.1762\n",
            "Epoch 25/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0588 - mae: 0.1916 - val_loss: 0.0498 - val_mae: 0.1820\n",
            "Epoch 26/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0538 - mae: 0.1836 - val_loss: 0.0440 - val_mae: 0.1715\n",
            "Epoch 27/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0512 - mae: 0.1839 - val_loss: 0.0445 - val_mae: 0.1701\n",
            "Epoch 28/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0539 - mae: 0.1867 - val_loss: 0.0534 - val_mae: 0.1881\n",
            "Epoch 29/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0554 - mae: 0.1866 - val_loss: 0.0495 - val_mae: 0.1803\n",
            "Epoch 30/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487 - mae: 0.1769 - val_loss: 0.0507 - val_mae: 0.1810\n",
            "Epoch 31/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0554 - mae: 0.1864 - val_loss: 0.0499 - val_mae: 0.1808\n",
            "Epoch 32/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0538 - mae: 0.1827 - val_loss: 0.0584 - val_mae: 0.1973\n",
            "Epoch 33/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0537 - mae: 0.1843 - val_loss: 0.0443 - val_mae: 0.1674\n",
            "Epoch 34/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0508 - mae: 0.1801 - val_loss: 0.0452 - val_mae: 0.1699\n",
            "Epoch 35/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0621 - mae: 0.1993 - val_loss: 0.0469 - val_mae: 0.1717\n",
            "Epoch 36/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0483 - mae: 0.1773 - val_loss: 0.0518 - val_mae: 0.1855\n",
            "Epoch 37/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0569 - mae: 0.1869 - val_loss: 0.0627 - val_mae: 0.2046\n",
            "Epoch 38/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0532 - mae: 0.1822 - val_loss: 0.0537 - val_mae: 0.1859\n",
            "Epoch 39/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0528 - mae: 0.1811 - val_loss: 0.0750 - val_mae: 0.2251\n",
            "Epoch 40/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0493 - mae: 0.1796 - val_loss: 0.0593 - val_mae: 0.1971\n",
            "Epoch 41/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0517 - mae: 0.1793 - val_loss: 0.0461 - val_mae: 0.1719\n",
            "Epoch 42/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0482 - mae: 0.1753 - val_loss: 0.0481 - val_mae: 0.1748\n",
            "Epoch 43/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0511 - mae: 0.1785 - val_loss: 0.0651 - val_mae: 0.2083\n",
            "Epoch 44/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0504 - mae: 0.1793 - val_loss: 0.0597 - val_mae: 0.1966\n",
            "Epoch 45/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492 - mae: 0.1810 - val_loss: 0.0554 - val_mae: 0.1904\n",
            "Epoch 46/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487 - mae: 0.1758 - val_loss: 0.0548 - val_mae: 0.1892\n",
            "Epoch 47/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0505 - mae: 0.1772 - val_loss: 0.0477 - val_mae: 0.1758\n",
            "Epoch 48/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489 - mae: 0.1768 - val_loss: 0.0478 - val_mae: 0.1742\n",
            "Epoch 49/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0461 - mae: 0.1717 - val_loss: 0.0485 - val_mae: 0.1788\n",
            "Epoch 50/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0544 - mae: 0.1858 - val_loss: 0.0500 - val_mae: 0.1765\n",
            "Epoch 51/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0488 - mae: 0.1761 - val_loss: 0.0498 - val_mae: 0.1779\n",
            "Epoch 52/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0478 - mae: 0.1722 - val_loss: 0.0705 - val_mae: 0.2172\n",
            "Epoch 53/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0465 - mae: 0.1697 - val_loss: 0.0464 - val_mae: 0.1726\n",
            "Epoch 54/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0471 - mae: 0.1727 - val_loss: 0.0481 - val_mae: 0.1758\n",
            "Epoch 55/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0476 - mae: 0.1752 - val_loss: 0.0533 - val_mae: 0.1823\n",
            "Epoch 56/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0481 - mae: 0.1769 - val_loss: 0.0497 - val_mae: 0.1785\n",
            "Epoch 57/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0481 - mae: 0.1727 - val_loss: 0.0488 - val_mae: 0.1775\n",
            "Epoch 58/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0465 - mae: 0.1734 - val_loss: 0.0579 - val_mae: 0.1937\n",
            "Epoch 59/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0470 - mae: 0.1720 - val_loss: 0.0499 - val_mae: 0.1790\n",
            "Epoch 60/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0459 - mae: 0.1712 - val_loss: 0.0481 - val_mae: 0.1759\n",
            "Epoch 61/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0429 - mae: 0.1669 - val_loss: 0.0474 - val_mae: 0.1765\n",
            "Epoch 62/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0462 - mae: 0.1706 - val_loss: 0.0648 - val_mae: 0.2092\n",
            "Epoch 63/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0463 - mae: 0.1730 - val_loss: 0.0463 - val_mae: 0.1749\n",
            "Epoch 64/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0449 - mae: 0.1685 - val_loss: 0.0475 - val_mae: 0.1746\n",
            "Epoch 65/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0414 - mae: 0.1597 - val_loss: 0.0471 - val_mae: 0.1747\n",
            "Epoch 66/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0466 - mae: 0.1704 - val_loss: 0.0500 - val_mae: 0.1787\n",
            "Epoch 67/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0473 - mae: 0.1701 - val_loss: 0.0616 - val_mae: 0.2025\n",
            "Epoch 68/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0480 - mae: 0.1774 - val_loss: 0.0629 - val_mae: 0.2040\n",
            "Epoch 69/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0443 - mae: 0.1671 - val_loss: 0.0562 - val_mae: 0.1924\n",
            "Epoch 70/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0437 - mae: 0.1678 - val_loss: 0.0498 - val_mae: 0.1781\n",
            "Epoch 71/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.1677 - val_loss: 0.0558 - val_mae: 0.1937\n",
            "Epoch 72/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0445 - mae: 0.1710 - val_loss: 0.0472 - val_mae: 0.1737\n",
            "Epoch 73/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0467 - mae: 0.1718 - val_loss: 0.0514 - val_mae: 0.1845\n",
            "Epoch 74/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0466 - mae: 0.1718 - val_loss: 0.0580 - val_mae: 0.1969\n",
            "Epoch 75/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0453 - mae: 0.1676 - val_loss: 0.0466 - val_mae: 0.1732\n",
            "Epoch 76/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0475 - mae: 0.1738 - val_loss: 0.0596 - val_mae: 0.1975\n",
            "Epoch 77/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0458 - mae: 0.1689 - val_loss: 0.0489 - val_mae: 0.1796\n",
            "Epoch 78/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0480 - mae: 0.1751 - val_loss: 0.0519 - val_mae: 0.1823\n",
            "Epoch 79/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0478 - mae: 0.1732 - val_loss: 0.0461 - val_mae: 0.1736\n",
            "Epoch 80/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0474 - mae: 0.1705 - val_loss: 0.0759 - val_mae: 0.2256\n",
            "Epoch 81/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0427 - mae: 0.1651 - val_loss: 0.0469 - val_mae: 0.1724\n",
            "Epoch 82/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0456 - mae: 0.1711 - val_loss: 0.0793 - val_mae: 0.2309\n",
            "Epoch 83/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0408 - mae: 0.1588 - val_loss: 0.0606 - val_mae: 0.2002\n",
            "Epoch 84/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0468 - mae: 0.1729 - val_loss: 0.0530 - val_mae: 0.1851\n",
            "Epoch 85/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0436 - mae: 0.1663 - val_loss: 0.0534 - val_mae: 0.1829\n",
            "Epoch 86/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1696 - val_loss: 0.0511 - val_mae: 0.1783\n",
            "Epoch 87/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0395 - mae: 0.1581 - val_loss: 0.0505 - val_mae: 0.1803\n",
            "Epoch 88/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0439 - mae: 0.1671 - val_loss: 0.0538 - val_mae: 0.1848\n",
            "Epoch 89/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0464 - mae: 0.1693 - val_loss: 0.0538 - val_mae: 0.1852\n",
            "Epoch 90/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0432 - mae: 0.1620 - val_loss: 0.0678 - val_mae: 0.2099\n",
            "Epoch 91/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0437 - mae: 0.1652 - val_loss: 0.0575 - val_mae: 0.1927\n",
            "Epoch 92/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0423 - mae: 0.1608 - val_loss: 0.0588 - val_mae: 0.1948\n",
            "Epoch 93/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0425 - mae: 0.1663 - val_loss: 0.0527 - val_mae: 0.1838\n",
            "Epoch 94/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0419 - mae: 0.1604 - val_loss: 0.0580 - val_mae: 0.1934\n",
            "Epoch 95/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0430 - mae: 0.1644 - val_loss: 0.0502 - val_mae: 0.1780\n",
            "Epoch 96/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0456 - mae: 0.1722 - val_loss: 0.0551 - val_mae: 0.1890\n",
            "Epoch 97/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0456 - mae: 0.1690 - val_loss: 0.0482 - val_mae: 0.1753\n",
            "Epoch 98/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0448 - mae: 0.1699 - val_loss: 0.0575 - val_mae: 0.1944\n",
            "Epoch 99/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0426 - mae: 0.1626 - val_loss: 0.0610 - val_mae: 0.2000\n",
            "Epoch 100/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0448 - mae: 0.1680 - val_loss: 0.0635 - val_mae: 0.2019\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "def create_model_1():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_1 = scaler.fit_transform(X_train_1)\n",
        "X_test_1 = scaler.transform(X_test_1)\n",
        "\n",
        "# Crear y entrenar el modelo 1\n",
        "model_1 = create_model_1()\n",
        "history_1 = model_1.fit(X_train_1, y_train_1, epochs=100, batch_size=20, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_1, test_mae_1 = model_1.evaluate(X_test_1, y_test_1)\n",
        "\n",
        "print(f\"Modelo 1 - Test Loss (MSE): {test_loss_1}, Test MAE: {test_mae_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW6fTmhZ_BcX",
        "outputId": "04c5b859-6e2f-4ce6-90e4-c1f43446817c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0645 - mae: 0.2060 \n",
            "Modelo 1 - Test Loss (MSE): 0.06357236951589584, Test MAE: 0.20485550165176392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_1 = model_1.predict(X_test_1)\n",
        "\n",
        "for i in range(len(predictions_1)):\n",
        "    print(f\"Predicted GPA: {predictions_1[i][0]:.4f}, Actual GPA: {y_test_1.iloc[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZzi221OBlKs",
        "outputId": "8ce3c562-e3fe-420b-cc55-7f39226c350f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "Predicted GPA: 1.6279, Actual GPA: 1.4277\n",
            "Predicted GPA: 2.8732, Actual GPA: 3.1174\n",
            "Predicted GPA: 1.9759, Actual GPA: 2.0378\n",
            "Predicted GPA: 3.2155, Actual GPA: 3.5485\n",
            "Predicted GPA: 0.4138, Actual GPA: 0.2490\n",
            "Predicted GPA: 2.5314, Actual GPA: 2.6277\n",
            "Predicted GPA: 1.5698, Actual GPA: 2.0574\n",
            "Predicted GPA: 2.0184, Actual GPA: 2.2483\n",
            "Predicted GPA: 1.9198, Actual GPA: 2.1947\n",
            "Predicted GPA: 1.0563, Actual GPA: 0.7582\n",
            "Predicted GPA: 2.4135, Actual GPA: 2.3709\n",
            "Predicted GPA: 0.7245, Actual GPA: 0.7664\n",
            "Predicted GPA: 2.8316, Actual GPA: 2.9527\n",
            "Predicted GPA: 2.5813, Actual GPA: 2.3433\n",
            "Predicted GPA: 2.6004, Actual GPA: 2.7718\n",
            "Predicted GPA: 0.4154, Actual GPA: 0.2879\n",
            "Predicted GPA: 1.0135, Actual GPA: 1.0183\n",
            "Predicted GPA: 1.3817, Actual GPA: 1.6294\n",
            "Predicted GPA: 1.9797, Actual GPA: 2.0744\n",
            "Predicted GPA: 2.5429, Actual GPA: 2.4238\n",
            "Predicted GPA: 2.0706, Actual GPA: 1.7562\n",
            "Predicted GPA: 1.6490, Actual GPA: 1.5663\n",
            "Predicted GPA: 1.6705, Actual GPA: 1.7062\n",
            "Predicted GPA: 2.9634, Actual GPA: 3.1614\n",
            "Predicted GPA: 1.9101, Actual GPA: 1.7334\n",
            "Predicted GPA: 0.6273, Actual GPA: 0.8420\n",
            "Predicted GPA: 1.6957, Actual GPA: 1.3792\n",
            "Predicted GPA: 2.4159, Actual GPA: 3.0270\n",
            "Predicted GPA: 1.9600, Actual GPA: 2.1920\n",
            "Predicted GPA: 1.7787, Actual GPA: 2.3158\n",
            "Predicted GPA: 1.6430, Actual GPA: 2.0681\n",
            "Predicted GPA: 0.7709, Actual GPA: 0.8691\n",
            "Predicted GPA: 2.7428, Actual GPA: 2.9001\n",
            "Predicted GPA: 2.9599, Actual GPA: 3.4686\n",
            "Predicted GPA: 1.5244, Actual GPA: 1.5674\n",
            "Predicted GPA: 1.8533, Actual GPA: 1.7947\n",
            "Predicted GPA: 2.9262, Actual GPA: 3.1813\n",
            "Predicted GPA: 2.8466, Actual GPA: 2.8974\n",
            "Predicted GPA: 2.9358, Actual GPA: 3.2449\n",
            "Predicted GPA: 0.7027, Actual GPA: 0.3578\n",
            "Predicted GPA: 2.4450, Actual GPA: 2.6524\n",
            "Predicted GPA: 3.1273, Actual GPA: 3.6810\n",
            "Predicted GPA: 1.2764, Actual GPA: 1.0364\n",
            "Predicted GPA: 2.1851, Actual GPA: 2.0172\n",
            "Predicted GPA: 0.9856, Actual GPA: 0.9634\n",
            "Predicted GPA: 2.4887, Actual GPA: 2.2395\n",
            "Predicted GPA: 2.7231, Actual GPA: 2.7360\n",
            "Predicted GPA: 1.0563, Actual GPA: 1.3619\n",
            "Predicted GPA: 2.6286, Actual GPA: 2.7030\n",
            "Predicted GPA: 1.4326, Actual GPA: 1.4419\n",
            "Predicted GPA: 2.8084, Actual GPA: 3.2195\n",
            "Predicted GPA: 2.8303, Actual GPA: 3.3391\n",
            "Predicted GPA: 1.7914, Actual GPA: 1.5562\n",
            "Predicted GPA: 1.3224, Actual GPA: 1.3424\n",
            "Predicted GPA: 1.8542, Actual GPA: 1.7562\n",
            "Predicted GPA: 3.1336, Actual GPA: 3.4218\n",
            "Predicted GPA: 2.4350, Actual GPA: 2.3170\n",
            "Predicted GPA: 3.0154, Actual GPA: 3.2866\n",
            "Predicted GPA: 1.0638, Actual GPA: 0.6847\n",
            "Predicted GPA: 2.0073, Actual GPA: 2.1371\n",
            "Predicted GPA: 1.8380, Actual GPA: 1.6885\n",
            "Predicted GPA: 1.7523, Actual GPA: 2.1962\n",
            "Predicted GPA: 2.3114, Actual GPA: 2.4778\n",
            "Predicted GPA: 1.5277, Actual GPA: 1.1979\n",
            "Predicted GPA: 1.1477, Actual GPA: 0.9882\n",
            "Predicted GPA: 2.2402, Actual GPA: 1.8589\n",
            "Predicted GPA: 3.1669, Actual GPA: 3.0407\n",
            "Predicted GPA: 2.6640, Actual GPA: 2.3741\n",
            "Predicted GPA: 1.2853, Actual GPA: 1.2213\n",
            "Predicted GPA: 3.2915, Actual GPA: 3.2742\n",
            "Predicted GPA: 3.2230, Actual GPA: 3.5452\n",
            "Predicted GPA: 0.9190, Actual GPA: 1.1019\n",
            "Predicted GPA: 2.7369, Actual GPA: 2.9688\n",
            "Predicted GPA: 2.7308, Actual GPA: 2.5762\n",
            "Predicted GPA: 0.8642, Actual GPA: 0.4541\n",
            "Predicted GPA: 2.7182, Actual GPA: 2.6742\n",
            "Predicted GPA: 1.7683, Actual GPA: 2.0600\n",
            "Predicted GPA: 1.5873, Actual GPA: 1.9831\n",
            "Predicted GPA: 0.4169, Actual GPA: 0.2124\n",
            "Predicted GPA: 1.4005, Actual GPA: 1.2794\n",
            "Predicted GPA: 1.8220, Actual GPA: 2.0297\n",
            "Predicted GPA: 2.8812, Actual GPA: 3.1892\n",
            "Predicted GPA: 2.1617, Actual GPA: 2.2565\n",
            "Predicted GPA: 1.6473, Actual GPA: 1.6364\n",
            "Predicted GPA: 1.4391, Actual GPA: 1.4016\n",
            "Predicted GPA: 0.5903, Actual GPA: 0.4436\n",
            "Predicted GPA: 3.5504, Actual GPA: 4.0000\n",
            "Predicted GPA: 1.2840, Actual GPA: 1.2712\n",
            "Predicted GPA: 1.3206, Actual GPA: 1.3026\n",
            "Predicted GPA: 1.7710, Actual GPA: 1.5970\n",
            "Predicted GPA: 2.4481, Actual GPA: 2.5354\n",
            "Predicted GPA: 1.4143, Actual GPA: 1.3829\n",
            "Predicted GPA: 1.0138, Actual GPA: 1.0385\n",
            "Predicted GPA: 0.8105, Actual GPA: 0.1307\n",
            "Predicted GPA: 1.6673, Actual GPA: 1.2738\n",
            "Predicted GPA: 1.0908, Actual GPA: 1.2715\n",
            "Predicted GPA: 3.1940, Actual GPA: 3.3251\n",
            "Predicted GPA: 1.2863, Actual GPA: 1.0707\n",
            "Predicted GPA: 1.6936, Actual GPA: 1.5743\n",
            "Predicted GPA: 1.7171, Actual GPA: 1.6337\n",
            "Predicted GPA: 0.5494, Actual GPA: 0.0000\n",
            "Predicted GPA: 2.2922, Actual GPA: 2.1221\n",
            "Predicted GPA: 1.2516, Actual GPA: 1.2912\n",
            "Predicted GPA: 1.6152, Actual GPA: 1.4547\n",
            "Predicted GPA: 0.9601, Actual GPA: 0.7743\n",
            "Predicted GPA: 2.1059, Actual GPA: 2.2931\n",
            "Predicted GPA: 2.5660, Actual GPA: 2.6828\n",
            "Predicted GPA: 1.2874, Actual GPA: 1.3155\n",
            "Predicted GPA: 1.4534, Actual GPA: 1.4694\n",
            "Predicted GPA: 0.6997, Actual GPA: 0.6132\n",
            "Predicted GPA: 1.7160, Actual GPA: 1.8820\n",
            "Predicted GPA: 2.0343, Actual GPA: 2.2689\n",
            "Predicted GPA: 3.0741, Actual GPA: 3.5452\n",
            "Predicted GPA: 0.5747, Actual GPA: 0.4258\n",
            "Predicted GPA: 0.6607, Actual GPA: 0.5404\n",
            "Predicted GPA: 1.3425, Actual GPA: 1.1111\n",
            "Predicted GPA: 2.1113, Actual GPA: 2.0354\n",
            "Predicted GPA: 2.6645, Actual GPA: 2.9955\n",
            "Predicted GPA: 0.6515, Actual GPA: 1.2179\n",
            "Predicted GPA: 1.9956, Actual GPA: 2.1270\n",
            "Predicted GPA: 0.7938, Actual GPA: 0.5298\n",
            "Predicted GPA: 2.8257, Actual GPA: 3.0169\n",
            "Predicted GPA: 3.0583, Actual GPA: 3.5921\n",
            "Predicted GPA: 0.9711, Actual GPA: 1.0583\n",
            "Predicted GPA: 1.8683, Actual GPA: 1.8426\n",
            "Predicted GPA: 1.0496, Actual GPA: 1.2546\n",
            "Predicted GPA: 2.3538, Actual GPA: 2.7949\n",
            "Predicted GPA: 1.0986, Actual GPA: 1.3479\n",
            "Predicted GPA: 1.9964, Actual GPA: 2.2270\n",
            "Predicted GPA: 2.6241, Actual GPA: 2.8135\n",
            "Predicted GPA: 0.9968, Actual GPA: 1.1056\n",
            "Predicted GPA: 3.8296, Actual GPA: 4.0000\n",
            "Predicted GPA: 2.7036, Actual GPA: 2.8982\n",
            "Predicted GPA: 0.5233, Actual GPA: 0.4240\n",
            "Predicted GPA: 0.8041, Actual GPA: 0.6915\n",
            "Predicted GPA: 0.7161, Actual GPA: 0.9300\n",
            "Predicted GPA: 1.9617, Actual GPA: 2.3308\n",
            "Predicted GPA: 0.4037, Actual GPA: 0.3376\n",
            "Predicted GPA: 0.3360, Actual GPA: 0.5022\n",
            "Predicted GPA: 1.1335, Actual GPA: 0.9893\n",
            "Predicted GPA: 2.4123, Actual GPA: 2.7459\n",
            "Predicted GPA: 1.9950, Actual GPA: 1.8042\n",
            "Predicted GPA: 2.8989, Actual GPA: 3.3433\n",
            "Predicted GPA: 1.8807, Actual GPA: 1.5731\n",
            "Predicted GPA: 2.7818, Actual GPA: 2.9640\n",
            "Predicted GPA: 1.3681, Actual GPA: 1.1936\n",
            "Predicted GPA: 1.4339, Actual GPA: 1.3493\n",
            "Predicted GPA: 2.1696, Actual GPA: 2.5202\n",
            "Predicted GPA: 2.8803, Actual GPA: 3.2538\n",
            "Predicted GPA: 1.3613, Actual GPA: 1.4634\n",
            "Predicted GPA: 2.9040, Actual GPA: 3.1620\n",
            "Predicted GPA: 0.8748, Actual GPA: 0.7696\n",
            "Predicted GPA: 2.9686, Actual GPA: 3.5037\n",
            "Predicted GPA: 1.4487, Actual GPA: 1.2688\n",
            "Predicted GPA: 1.1211, Actual GPA: 1.1708\n",
            "Predicted GPA: 0.6271, Actual GPA: 0.4713\n",
            "Predicted GPA: 1.4185, Actual GPA: 1.3979\n",
            "Predicted GPA: 0.7273, Actual GPA: 0.9690\n",
            "Predicted GPA: 1.4488, Actual GPA: 1.5113\n",
            "Predicted GPA: 0.6987, Actual GPA: 0.5242\n",
            "Predicted GPA: 1.5867, Actual GPA: 1.4914\n",
            "Predicted GPA: 1.2162, Actual GPA: 1.5194\n",
            "Predicted GPA: 0.8418, Actual GPA: 0.7133\n",
            "Predicted GPA: 0.7979, Actual GPA: 0.4006\n",
            "Predicted GPA: 0.5103, Actual GPA: 0.5814\n",
            "Predicted GPA: 0.6903, Actual GPA: 0.4051\n",
            "Predicted GPA: 0.7955, Actual GPA: 1.1544\n",
            "Predicted GPA: 1.3103, Actual GPA: 1.1502\n",
            "Predicted GPA: 2.3018, Actual GPA: 2.4579\n",
            "Predicted GPA: 3.1066, Actual GPA: 3.4555\n",
            "Predicted GPA: 1.6169, Actual GPA: 1.4414\n",
            "Predicted GPA: 3.2519, Actual GPA: 3.2880\n",
            "Predicted GPA: 2.8137, Actual GPA: 3.0439\n",
            "Predicted GPA: 1.2490, Actual GPA: 0.1898\n",
            "Predicted GPA: 1.3559, Actual GPA: 1.1658\n",
            "Predicted GPA: 2.3309, Actual GPA: 2.5654\n",
            "Predicted GPA: 2.4281, Actual GPA: 2.2347\n",
            "Predicted GPA: 1.3332, Actual GPA: 1.2587\n",
            "Predicted GPA: 2.0524, Actual GPA: 2.1167\n",
            "Predicted GPA: 1.6428, Actual GPA: 1.6747\n",
            "Predicted GPA: 2.8991, Actual GPA: 2.8211\n",
            "Predicted GPA: 1.7715, Actual GPA: 1.9961\n",
            "Predicted GPA: 0.8396, Actual GPA: 0.8091\n",
            "Predicted GPA: 2.8132, Actual GPA: 2.5466\n",
            "Predicted GPA: 2.6993, Actual GPA: 2.7350\n",
            "Predicted GPA: 2.6959, Actual GPA: 2.7536\n",
            "Predicted GPA: 3.0221, Actual GPA: 3.2839\n",
            "Predicted GPA: 1.5019, Actual GPA: 1.2878\n",
            "Predicted GPA: 0.4259, Actual GPA: 0.3107\n",
            "Predicted GPA: 1.8985, Actual GPA: 1.8315\n",
            "Predicted GPA: 1.3329, Actual GPA: 0.9116\n",
            "Predicted GPA: 1.9378, Actual GPA: 2.1097\n",
            "Predicted GPA: 1.2609, Actual GPA: 1.0445\n",
            "Predicted GPA: 0.7426, Actual GPA: 0.9655\n",
            "Predicted GPA: 1.9366, Actual GPA: 1.5504\n",
            "Predicted GPA: 1.4781, Actual GPA: 1.2238\n",
            "Predicted GPA: 1.8509, Actual GPA: 1.8934\n",
            "Predicted GPA: 1.9938, Actual GPA: 2.2954\n",
            "Predicted GPA: 2.6341, Actual GPA: 2.8393\n",
            "Predicted GPA: 2.5668, Actual GPA: 2.2518\n",
            "Predicted GPA: 1.4313, Actual GPA: 1.4053\n",
            "Predicted GPA: 2.8164, Actual GPA: 3.0321\n",
            "Predicted GPA: 0.6626, Actual GPA: 0.5694\n",
            "Predicted GPA: 2.1833, Actual GPA: 2.2201\n",
            "Predicted GPA: 2.8530, Actual GPA: 2.7374\n",
            "Predicted GPA: 1.9349, Actual GPA: 2.3408\n",
            "Predicted GPA: 0.5817, Actual GPA: 0.5602\n",
            "Predicted GPA: 2.8763, Actual GPA: 2.9139\n",
            "Predicted GPA: 1.9877, Actual GPA: 2.2595\n",
            "Predicted GPA: 1.4991, Actual GPA: 1.7420\n",
            "Predicted GPA: 1.4468, Actual GPA: 1.2659\n",
            "Predicted GPA: 0.8970, Actual GPA: 1.2987\n",
            "Predicted GPA: 2.1040, Actual GPA: 1.9138\n",
            "Predicted GPA: 2.5477, Actual GPA: 2.7808\n",
            "Predicted GPA: 2.2931, Actual GPA: 2.5535\n",
            "Predicted GPA: 0.9867, Actual GPA: 0.8923\n",
            "Predicted GPA: 2.7587, Actual GPA: 2.6697\n",
            "Predicted GPA: 2.8714, Actual GPA: 3.0885\n",
            "Predicted GPA: 1.8354, Actual GPA: 1.5636\n",
            "Predicted GPA: 2.6875, Actual GPA: 2.8757\n",
            "Predicted GPA: 1.1716, Actual GPA: 1.1845\n",
            "Predicted GPA: 2.3501, Actual GPA: 2.5842\n",
            "Predicted GPA: 3.1977, Actual GPA: 3.8128\n",
            "Predicted GPA: 0.5225, Actual GPA: 0.6729\n",
            "Predicted GPA: 1.6221, Actual GPA: 1.9559\n",
            "Predicted GPA: 1.6675, Actual GPA: 1.7733\n",
            "Predicted GPA: 2.9805, Actual GPA: 3.0554\n",
            "Predicted GPA: 2.5429, Actual GPA: 2.7185\n",
            "Predicted GPA: 1.2194, Actual GPA: 1.2705\n",
            "Predicted GPA: 2.0551, Actual GPA: 1.9880\n",
            "Predicted GPA: 1.4592, Actual GPA: 1.7209\n",
            "Predicted GPA: 1.8444, Actual GPA: 2.1106\n",
            "Predicted GPA: 2.5505, Actual GPA: 2.8816\n",
            "Predicted GPA: 0.9584, Actual GPA: 0.9708\n",
            "Predicted GPA: 2.8196, Actual GPA: 2.9729\n",
            "Predicted GPA: 2.5015, Actual GPA: 2.5210\n",
            "Predicted GPA: 0.7600, Actual GPA: 0.5152\n",
            "Predicted GPA: 1.1212, Actual GPA: 1.0086\n",
            "Predicted GPA: 2.6161, Actual GPA: 2.7913\n",
            "Predicted GPA: 0.8475, Actual GPA: 0.5868\n",
            "Predicted GPA: 3.2169, Actual GPA: 3.6663\n",
            "Predicted GPA: 1.1521, Actual GPA: 0.9031\n",
            "Predicted GPA: 0.4914, Actual GPA: 0.7046\n",
            "Predicted GPA: 2.2829, Actual GPA: 2.3983\n",
            "Predicted GPA: 1.4528, Actual GPA: 1.8178\n",
            "Predicted GPA: 0.7256, Actual GPA: 0.7996\n",
            "Predicted GPA: 2.3823, Actual GPA: 2.1274\n",
            "Predicted GPA: 1.5838, Actual GPA: 1.1768\n",
            "Predicted GPA: 0.5741, Actual GPA: 0.4291\n",
            "Predicted GPA: 1.6195, Actual GPA: 2.0469\n",
            "Predicted GPA: 1.2811, Actual GPA: 1.0862\n",
            "Predicted GPA: 2.8833, Actual GPA: 2.9869\n",
            "Predicted GPA: 2.4285, Actual GPA: 2.4740\n",
            "Predicted GPA: 1.9960, Actual GPA: 1.5819\n",
            "Predicted GPA: 2.1459, Actual GPA: 2.3278\n",
            "Predicted GPA: 1.6298, Actual GPA: 1.5253\n",
            "Predicted GPA: 1.4281, Actual GPA: 1.6831\n",
            "Predicted GPA: 2.2286, Actual GPA: 2.4610\n",
            "Predicted GPA: 0.5034, Actual GPA: 0.0278\n",
            "Predicted GPA: 1.5416, Actual GPA: 1.5364\n",
            "Predicted GPA: 2.0629, Actual GPA: 2.0999\n",
            "Predicted GPA: 0.6083, Actual GPA: 0.3751\n",
            "Predicted GPA: 1.4868, Actual GPA: 1.2946\n",
            "Predicted GPA: 0.7300, Actual GPA: 0.7697\n",
            "Predicted GPA: 1.9449, Actual GPA: 1.8214\n",
            "Predicted GPA: 3.0932, Actual GPA: 3.4983\n",
            "Predicted GPA: 2.4382, Actual GPA: 2.3781\n",
            "Predicted GPA: 0.8766, Actual GPA: 0.5185\n",
            "Predicted GPA: 2.4282, Actual GPA: 2.7549\n",
            "Predicted GPA: 2.1140, Actual GPA: 2.2070\n",
            "Predicted GPA: 1.1667, Actual GPA: 1.1416\n",
            "Predicted GPA: 3.0828, Actual GPA: 3.1376\n",
            "Predicted GPA: 1.5143, Actual GPA: 1.3638\n",
            "Predicted GPA: 1.4054, Actual GPA: 1.6885\n",
            "Predicted GPA: 1.3377, Actual GPA: 1.3503\n",
            "Predicted GPA: 2.5113, Actual GPA: 2.0625\n",
            "Predicted GPA: 1.1473, Actual GPA: 1.0226\n",
            "Predicted GPA: 1.8922, Actual GPA: 1.7923\n",
            "Predicted GPA: 0.3019, Actual GPA: 0.0000\n",
            "Predicted GPA: 2.5958, Actual GPA: 2.8729\n",
            "Predicted GPA: 2.9495, Actual GPA: 3.0887\n",
            "Predicted GPA: 2.5833, Actual GPA: 2.9375\n",
            "Predicted GPA: 1.9766, Actual GPA: 1.9901\n",
            "Predicted GPA: 1.4958, Actual GPA: 1.5165\n",
            "Predicted GPA: 2.8845, Actual GPA: 3.3009\n",
            "Predicted GPA: 2.3724, Actual GPA: 2.1727\n",
            "Predicted GPA: 1.2849, Actual GPA: 1.1194\n",
            "Predicted GPA: 1.4612, Actual GPA: 1.5718\n",
            "Predicted GPA: 0.8113, Actual GPA: 0.4959\n",
            "Predicted GPA: 0.5894, Actual GPA: 0.1550\n",
            "Predicted GPA: 3.2419, Actual GPA: 3.8650\n",
            "Predicted GPA: 1.7709, Actual GPA: 1.8044\n",
            "Predicted GPA: 2.3908, Actual GPA: 2.7842\n",
            "Predicted GPA: 1.2328, Actual GPA: 1.2059\n",
            "Predicted GPA: 2.3937, Actual GPA: 2.8236\n",
            "Predicted GPA: 1.1673, Actual GPA: 0.8952\n",
            "Predicted GPA: 2.7528, Actual GPA: 3.1123\n",
            "Predicted GPA: 1.6741, Actual GPA: 1.8154\n",
            "Predicted GPA: 0.4091, Actual GPA: 0.1006\n",
            "Predicted GPA: 2.7829, Actual GPA: 3.0655\n",
            "Predicted GPA: 2.6059, Actual GPA: 2.5539\n",
            "Predicted GPA: 1.8867, Actual GPA: 1.9864\n",
            "Predicted GPA: 1.0828, Actual GPA: 0.9941\n",
            "Predicted GPA: 3.0812, Actual GPA: 3.3501\n",
            "Predicted GPA: 2.2441, Actual GPA: 2.3117\n",
            "Predicted GPA: 2.4714, Actual GPA: 2.5042\n",
            "Predicted GPA: 0.6132, Actual GPA: 0.7635\n",
            "Predicted GPA: 2.2773, Actual GPA: 2.7122\n",
            "Predicted GPA: 0.9570, Actual GPA: 0.9253\n",
            "Predicted GPA: 1.9551, Actual GPA: 1.8498\n",
            "Predicted GPA: 1.3804, Actual GPA: 1.3971\n",
            "Predicted GPA: 1.5885, Actual GPA: 1.8890\n",
            "Predicted GPA: 0.8016, Actual GPA: 0.9000\n",
            "Predicted GPA: 1.0398, Actual GPA: 1.0277\n",
            "Predicted GPA: 0.9015, Actual GPA: 0.7017\n",
            "Predicted GPA: 1.4609, Actual GPA: 1.4120\n",
            "Predicted GPA: 2.7746, Actual GPA: 2.9774\n",
            "Predicted GPA: 2.2446, Actual GPA: 2.2104\n",
            "Predicted GPA: 2.5049, Actual GPA: 2.4387\n",
            "Predicted GPA: 1.8102, Actual GPA: 1.9213\n",
            "Predicted GPA: 2.0107, Actual GPA: 1.7340\n",
            "Predicted GPA: 1.2267, Actual GPA: 1.5893\n",
            "Predicted GPA: 1.8182, Actual GPA: 1.5034\n",
            "Predicted GPA: 2.0941, Actual GPA: 2.2220\n",
            "Predicted GPA: 2.8472, Actual GPA: 3.1062\n",
            "Predicted GPA: 1.3661, Actual GPA: 1.5421\n",
            "Predicted GPA: 1.0289, Actual GPA: 1.2523\n",
            "Predicted GPA: 1.3696, Actual GPA: 1.7105\n",
            "Predicted GPA: 2.5010, Actual GPA: 2.6936\n",
            "Predicted GPA: 0.9709, Actual GPA: 0.9657\n",
            "Predicted GPA: 1.8574, Actual GPA: 1.8683\n",
            "Predicted GPA: 2.8117, Actual GPA: 2.8069\n",
            "Predicted GPA: 2.2506, Actual GPA: 2.3428\n",
            "Predicted GPA: 2.5228, Actual GPA: 2.5837\n",
            "Predicted GPA: 2.9739, Actual GPA: 3.2486\n",
            "Predicted GPA: 2.4448, Actual GPA: 2.5845\n",
            "Predicted GPA: 2.4135, Actual GPA: 2.5876\n",
            "Predicted GPA: 1.4052, Actual GPA: 1.5660\n",
            "Predicted GPA: 0.8982, Actual GPA: 0.8670\n",
            "Predicted GPA: 1.7500, Actual GPA: 1.8476\n",
            "Predicted GPA: 1.4619, Actual GPA: 1.1221\n",
            "Predicted GPA: 1.6358, Actual GPA: 1.7182\n",
            "Predicted GPA: 1.3887, Actual GPA: 1.1645\n",
            "Predicted GPA: 1.7205, Actual GPA: 1.6833\n",
            "Predicted GPA: 2.0216, Actual GPA: 2.0186\n",
            "Predicted GPA: 2.3279, Actual GPA: 2.5125\n",
            "Predicted GPA: 2.7976, Actual GPA: 3.1690\n",
            "Predicted GPA: 3.0904, Actual GPA: 3.0526\n",
            "Predicted GPA: 1.0086, Actual GPA: 1.1529\n",
            "Predicted GPA: 2.9521, Actual GPA: 3.2583\n",
            "Predicted GPA: 3.0911, Actual GPA: 3.3637\n",
            "Predicted GPA: 2.1935, Actual GPA: 2.4041\n",
            "Predicted GPA: 2.0246, Actual GPA: 2.1377\n",
            "Predicted GPA: 2.4677, Actual GPA: 2.4276\n",
            "Predicted GPA: 1.7946, Actual GPA: 1.8100\n",
            "Predicted GPA: 1.8299, Actual GPA: 1.7560\n",
            "Predicted GPA: 2.2767, Actual GPA: 2.0531\n",
            "Predicted GPA: 1.1352, Actual GPA: 1.6015\n",
            "Predicted GPA: 0.7886, Actual GPA: 0.4696\n",
            "Predicted GPA: 2.2754, Actual GPA: 2.0046\n",
            "Predicted GPA: 2.4800, Actual GPA: 2.2888\n",
            "Predicted GPA: 2.8688, Actual GPA: 3.0917\n",
            "Predicted GPA: 2.2313, Actual GPA: 2.1172\n",
            "Predicted GPA: 1.1113, Actual GPA: 1.3476\n",
            "Predicted GPA: 1.7733, Actual GPA: 1.5304\n",
            "Predicted GPA: 2.6146, Actual GPA: 2.7344\n",
            "Predicted GPA: 3.4110, Actual GPA: 3.8302\n",
            "Predicted GPA: 2.4854, Actual GPA: 2.6394\n",
            "Predicted GPA: 2.5071, Actual GPA: 2.4714\n",
            "Predicted GPA: 2.0868, Actual GPA: 2.2286\n",
            "Predicted GPA: 0.7904, Actual GPA: 0.6566\n",
            "Predicted GPA: 2.0777, Actual GPA: 2.2598\n",
            "Predicted GPA: 2.3227, Actual GPA: 2.3328\n",
            "Predicted GPA: 2.5446, Actual GPA: 3.0235\n",
            "Predicted GPA: 2.4302, Actual GPA: 2.9647\n",
            "Predicted GPA: 1.1292, Actual GPA: 0.9836\n",
            "Predicted GPA: 1.3929, Actual GPA: 1.4655\n",
            "Predicted GPA: 2.2164, Actual GPA: 2.2689\n",
            "Predicted GPA: 3.2676, Actual GPA: 3.5729\n",
            "Predicted GPA: 0.9429, Actual GPA: 0.6823\n",
            "Predicted GPA: 1.9294, Actual GPA: 1.9847\n",
            "Predicted GPA: 2.2445, Actual GPA: 2.5197\n",
            "Predicted GPA: 0.8060, Actual GPA: 0.5496\n",
            "Predicted GPA: 2.7625, Actual GPA: 2.9869\n",
            "Predicted GPA: 2.6252, Actual GPA: 2.6405\n",
            "Predicted GPA: 1.7599, Actual GPA: 1.4773\n",
            "Predicted GPA: 2.2280, Actual GPA: 1.9396\n",
            "Predicted GPA: 1.9405, Actual GPA: 1.8298\n",
            "Predicted GPA: 2.4747, Actual GPA: 2.6696\n",
            "Predicted GPA: 1.7373, Actual GPA: 1.5961\n",
            "Predicted GPA: 2.3626, Actual GPA: 2.2790\n",
            "Predicted GPA: 0.9484, Actual GPA: 1.1378\n",
            "Predicted GPA: 0.9889, Actual GPA: 0.6768\n",
            "Predicted GPA: 2.2110, Actual GPA: 2.5040\n",
            "Predicted GPA: 2.0596, Actual GPA: 2.1226\n",
            "Predicted GPA: 2.7120, Actual GPA: 2.9779\n",
            "Predicted GPA: 2.6796, Actual GPA: 2.8276\n",
            "Predicted GPA: 2.8154, Actual GPA: 2.9422\n",
            "Predicted GPA: 1.9411, Actual GPA: 2.3100\n",
            "Predicted GPA: 2.6302, Actual GPA: 2.7779\n",
            "Predicted GPA: 2.3483, Actual GPA: 2.5244\n",
            "Predicted GPA: 2.1757, Actual GPA: 2.1353\n",
            "Predicted GPA: 2.3841, Actual GPA: 2.2242\n",
            "Predicted GPA: 2.7356, Actual GPA: 3.2389\n",
            "Predicted GPA: 2.0161, Actual GPA: 2.2303\n",
            "Predicted GPA: 2.6199, Actual GPA: 3.0608\n",
            "Predicted GPA: 1.1065, Actual GPA: 1.3636\n",
            "Predicted GPA: 2.9110, Actual GPA: 3.6035\n",
            "Predicted GPA: 2.0144, Actual GPA: 2.1673\n",
            "Predicted GPA: 3.0533, Actual GPA: 3.3239\n",
            "Predicted GPA: 1.9348, Actual GPA: 2.0166\n",
            "Predicted GPA: 1.6460, Actual GPA: 1.3620\n",
            "Predicted GPA: 2.1242, Actual GPA: 2.1156\n",
            "Predicted GPA: 2.2787, Actual GPA: 2.4653\n",
            "Predicted GPA: 2.9753, Actual GPA: 3.0605\n",
            "Predicted GPA: 2.0890, Actual GPA: 1.9915\n",
            "Predicted GPA: 0.4612, Actual GPA: 0.2649\n",
            "Predicted GPA: 3.0542, Actual GPA: 3.6457\n",
            "Predicted GPA: 2.8106, Actual GPA: 2.5172\n",
            "Predicted GPA: 1.5216, Actual GPA: 1.5487\n",
            "Predicted GPA: 1.4179, Actual GPA: 1.5996\n",
            "Predicted GPA: 1.5107, Actual GPA: 1.5539\n",
            "Predicted GPA: 2.1075, Actual GPA: 2.5959\n",
            "Predicted GPA: 1.8592, Actual GPA: 1.8791\n",
            "Predicted GPA: 1.2134, Actual GPA: 1.5067\n",
            "Predicted GPA: 1.3061, Actual GPA: 1.1049\n",
            "Predicted GPA: 0.5168, Actual GPA: 0.3414\n",
            "Predicted GPA: 2.1251, Actual GPA: 2.4053\n",
            "Predicted GPA: 2.0196, Actual GPA: 1.6848\n",
            "Predicted GPA: 2.9508, Actual GPA: 2.9935\n",
            "Predicted GPA: 2.7210, Actual GPA: 3.4154\n",
            "Predicted GPA: 1.4388, Actual GPA: 1.7097\n",
            "Predicted GPA: 1.7760, Actual GPA: 1.5387\n",
            "Predicted GPA: 2.8300, Actual GPA: 2.9214\n",
            "Predicted GPA: 2.1069, Actual GPA: 2.6052\n",
            "Predicted GPA: 2.0938, Actual GPA: 2.4400\n",
            "Predicted GPA: 1.5518, Actual GPA: 1.4290\n",
            "Predicted GPA: 3.1592, Actual GPA: 3.3721\n",
            "Predicted GPA: 2.3417, Actual GPA: 2.1516\n",
            "Predicted GPA: 1.6485, Actual GPA: 1.5951\n",
            "Predicted GPA: 1.4903, Actual GPA: 1.4146\n",
            "Predicted GPA: 2.7787, Actual GPA: 3.1289\n",
            "Predicted GPA: 2.7714, Actual GPA: 2.5534\n",
            "Predicted GPA: 2.9043, Actual GPA: 3.1296\n",
            "Predicted GPA: 2.7263, Actual GPA: 2.9820\n",
            "Predicted GPA: 2.7937, Actual GPA: 2.9818\n",
            "Predicted GPA: 1.1038, Actual GPA: 1.2011\n",
            "Predicted GPA: 2.4661, Actual GPA: 2.7914\n",
            "Predicted GPA: 0.6951, Actual GPA: 0.1530\n",
            "Predicted GPA: 0.8122, Actual GPA: 0.6194\n",
            "Predicted GPA: 2.4000, Actual GPA: 2.2813\n",
            "Predicted GPA: 0.7352, Actual GPA: 0.4937\n",
            "Predicted GPA: 2.5834, Actual GPA: 2.9665\n",
            "Predicted GPA: 1.9359, Actual GPA: 1.9895\n",
            "Predicted GPA: 3.2141, Actual GPA: 3.5433\n",
            "Predicted GPA: 1.9578, Actual GPA: 1.9706\n",
            "Predicted GPA: 1.2226, Actual GPA: 1.3441\n",
            "Predicted GPA: 1.0160, Actual GPA: 1.0270\n",
            "Predicted GPA: 0.7389, Actual GPA: 0.4278\n",
            "Predicted GPA: 2.1963, Actual GPA: 1.9747\n",
            "Predicted GPA: 0.8425, Actual GPA: 1.2422\n",
            "Predicted GPA: 0.7821, Actual GPA: 0.3844\n",
            "Predicted GPA: 0.7717, Actual GPA: 0.4475\n",
            "Predicted GPA: 1.6596, Actual GPA: 1.7291\n",
            "Predicted GPA: 2.0645, Actual GPA: 2.1364\n",
            "Predicted GPA: 0.7615, Actual GPA: 0.2110\n",
            "Predicted GPA: 0.4422, Actual GPA: 0.3310\n",
            "Predicted GPA: 2.5501, Actual GPA: 2.8889\n",
            "Predicted GPA: 2.9896, Actual GPA: 3.2704\n",
            "Predicted GPA: 2.5855, Actual GPA: 2.5001\n",
            "Predicted GPA: 1.4310, Actual GPA: 1.2129\n",
            "Predicted GPA: 1.7957, Actual GPA: 2.0093\n",
            "Predicted GPA: 1.1175, Actual GPA: 1.5438\n",
            "Predicted GPA: 1.3059, Actual GPA: 1.4382\n",
            "Predicted GPA: 1.8415, Actual GPA: 1.5624\n",
            "Predicted GPA: 2.0059, Actual GPA: 2.1749\n",
            "Predicted GPA: 2.4153, Actual GPA: 2.3325\n",
            "Predicted GPA: 2.3734, Actual GPA: 2.7780\n",
            "Predicted GPA: 0.9338, Actual GPA: 0.8635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97E85iltvsND"
      },
      "source": [
        "#### Model 3:\n",
        "- Changes:\n",
        "   - Dataset Data Engineering\n",
        "   - Model Definition\n",
        "   - Model Compile\n",
        "   - Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "MdGp1BuavsND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c205b80-f7cb-478f-84fb-d7dc5c98fbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.2326 - mae: 1.6964 - val_loss: 0.8705 - val_mae: 1.2985\n",
            "Epoch 2/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7468 - mae: 1.1644 - val_loss: 0.5156 - val_mae: 0.9096\n",
            "Epoch 3/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4139 - mae: 0.7873 - val_loss: 0.3293 - val_mae: 0.6925\n",
            "Epoch 4/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2787 - mae: 0.6211 - val_loss: 0.2336 - val_mae: 0.5744\n",
            "Epoch 5/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2035 - mae: 0.5249 - val_loss: 0.1743 - val_mae: 0.4928\n",
            "Epoch 6/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1465 - mae: 0.4361 - val_loss: 0.1337 - val_mae: 0.4299\n",
            "Epoch 7/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1098 - mae: 0.3751 - val_loss: 0.1064 - val_mae: 0.3829\n",
            "Epoch 8/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0979 - mae: 0.3540 - val_loss: 0.0887 - val_mae: 0.3481\n",
            "Epoch 9/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0787 - mae: 0.3168 - val_loss: 0.0771 - val_mae: 0.3231\n",
            "Epoch 10/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0770 - mae: 0.3173 - val_loss: 0.0695 - val_mae: 0.3055\n",
            "Epoch 11/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0665 - mae: 0.2961 - val_loss: 0.0644 - val_mae: 0.2935\n",
            "Epoch 12/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0616 - mae: 0.2841 - val_loss: 0.0606 - val_mae: 0.2843\n",
            "Epoch 13/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0601 - mae: 0.2827 - val_loss: 0.0581 - val_mae: 0.2789\n",
            "Epoch 14/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0552 - mae: 0.2719 - val_loss: 0.0561 - val_mae: 0.2741\n",
            "Epoch 15/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0532 - mae: 0.2646 - val_loss: 0.0546 - val_mae: 0.2703\n",
            "Epoch 16/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0535 - mae: 0.2680 - val_loss: 0.0532 - val_mae: 0.2663\n",
            "Epoch 17/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0523 - mae: 0.2612 - val_loss: 0.0521 - val_mae: 0.2638\n",
            "Epoch 18/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0516 - mae: 0.2621 - val_loss: 0.0512 - val_mae: 0.2613\n",
            "Epoch 19/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0509 - mae: 0.2603 - val_loss: 0.0503 - val_mae: 0.2593\n",
            "Epoch 20/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0492 - mae: 0.2553 - val_loss: 0.0494 - val_mae: 0.2567\n",
            "Epoch 21/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0493 - mae: 0.2546 - val_loss: 0.0487 - val_mae: 0.2547\n",
            "Epoch 22/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0485 - mae: 0.2517 - val_loss: 0.0479 - val_mae: 0.2531\n",
            "Epoch 23/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0476 - mae: 0.2493 - val_loss: 0.0473 - val_mae: 0.2514\n",
            "Epoch 24/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0461 - mae: 0.2472 - val_loss: 0.0467 - val_mae: 0.2495\n",
            "Epoch 25/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0450 - mae: 0.2437 - val_loss: 0.0461 - val_mae: 0.2485\n",
            "Epoch 26/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0479 - mae: 0.2518 - val_loss: 0.0455 - val_mae: 0.2470\n",
            "Epoch 27/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0447 - mae: 0.2409 - val_loss: 0.0450 - val_mae: 0.2453\n",
            "Epoch 28/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0432 - mae: 0.2360 - val_loss: 0.0444 - val_mae: 0.2441\n",
            "Epoch 29/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0437 - mae: 0.2391 - val_loss: 0.0439 - val_mae: 0.2425\n",
            "Epoch 30/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0406 - mae: 0.2336 - val_loss: 0.0435 - val_mae: 0.2413\n",
            "Epoch 31/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0441 - mae: 0.2421 - val_loss: 0.0430 - val_mae: 0.2403\n",
            "Epoch 32/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0435 - mae: 0.2378 - val_loss: 0.0426 - val_mae: 0.2387\n",
            "Epoch 33/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.2400 - val_loss: 0.0421 - val_mae: 0.2376\n",
            "Epoch 34/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0418 - mae: 0.2350 - val_loss: 0.0417 - val_mae: 0.2363\n",
            "Epoch 35/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0402 - mae: 0.2297 - val_loss: 0.0413 - val_mae: 0.2354\n",
            "Epoch 36/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0419 - mae: 0.2348 - val_loss: 0.0409 - val_mae: 0.2342\n",
            "Epoch 37/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0405 - mae: 0.2308 - val_loss: 0.0406 - val_mae: 0.2333\n",
            "Epoch 38/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0417 - mae: 0.2340 - val_loss: 0.0402 - val_mae: 0.2320\n",
            "Epoch 39/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0423 - mae: 0.2345 - val_loss: 0.0398 - val_mae: 0.2310\n",
            "Epoch 40/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0412 - mae: 0.2355 - val_loss: 0.0395 - val_mae: 0.2298\n",
            "Epoch 41/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0385 - mae: 0.2242 - val_loss: 0.0392 - val_mae: 0.2290\n",
            "Epoch 42/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0372 - mae: 0.2218 - val_loss: 0.0388 - val_mae: 0.2281\n",
            "Epoch 43/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0372 - mae: 0.2202 - val_loss: 0.0385 - val_mae: 0.2270\n",
            "Epoch 44/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0381 - mae: 0.2262 - val_loss: 0.0382 - val_mae: 0.2261\n",
            "Epoch 45/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0355 - mae: 0.2163 - val_loss: 0.0380 - val_mae: 0.2251\n",
            "Epoch 46/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0360 - mae: 0.2164 - val_loss: 0.0377 - val_mae: 0.2244\n",
            "Epoch 47/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0369 - mae: 0.2225 - val_loss: 0.0374 - val_mae: 0.2236\n",
            "Epoch 48/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0378 - mae: 0.2252 - val_loss: 0.0371 - val_mae: 0.2230\n",
            "Epoch 49/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0400 - mae: 0.2282 - val_loss: 0.0369 - val_mae: 0.2219\n",
            "Epoch 50/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0338 - mae: 0.2089 - val_loss: 0.0366 - val_mae: 0.2212\n",
            "Epoch 51/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0353 - mae: 0.2149 - val_loss: 0.0364 - val_mae: 0.2205\n",
            "Epoch 52/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0353 - mae: 0.2160 - val_loss: 0.0362 - val_mae: 0.2195\n",
            "Epoch 53/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0352 - mae: 0.2140 - val_loss: 0.0359 - val_mae: 0.2189\n",
            "Epoch 54/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0347 - mae: 0.2137 - val_loss: 0.0357 - val_mae: 0.2182\n",
            "Epoch 55/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0337 - mae: 0.2084 - val_loss: 0.0355 - val_mae: 0.2175\n",
            "Epoch 56/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0349 - mae: 0.2143 - val_loss: 0.0353 - val_mae: 0.2168\n",
            "Epoch 57/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0339 - mae: 0.2099 - val_loss: 0.0351 - val_mae: 0.2161\n",
            "Epoch 58/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0344 - mae: 0.2128 - val_loss: 0.0349 - val_mae: 0.2156\n",
            "Epoch 59/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0331 - mae: 0.2100 - val_loss: 0.0347 - val_mae: 0.2153\n",
            "Epoch 60/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0350 - mae: 0.2150 - val_loss: 0.0345 - val_mae: 0.2143\n",
            "Epoch 61/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0331 - mae: 0.2092 - val_loss: 0.0343 - val_mae: 0.2136\n",
            "Epoch 62/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0347 - mae: 0.2128 - val_loss: 0.0341 - val_mae: 0.2131\n",
            "Epoch 63/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0338 - mae: 0.2127 - val_loss: 0.0339 - val_mae: 0.2127\n",
            "Epoch 64/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0339 - mae: 0.2142 - val_loss: 0.0338 - val_mae: 0.2120\n",
            "Epoch 65/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0347 - mae: 0.2164 - val_loss: 0.0336 - val_mae: 0.2114\n",
            "Epoch 66/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0340 - mae: 0.2122 - val_loss: 0.0334 - val_mae: 0.2109\n",
            "Epoch 67/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0323 - mae: 0.2060 - val_loss: 0.0333 - val_mae: 0.2102\n",
            "Epoch 68/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0324 - mae: 0.2071 - val_loss: 0.0331 - val_mae: 0.2096\n",
            "Epoch 69/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0324 - mae: 0.2074 - val_loss: 0.0330 - val_mae: 0.2092\n",
            "Epoch 70/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0324 - mae: 0.2076 - val_loss: 0.0328 - val_mae: 0.2087\n",
            "Epoch 71/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0331 - mae: 0.2084 - val_loss: 0.0327 - val_mae: 0.2082\n",
            "Epoch 72/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0335 - mae: 0.2104 - val_loss: 0.0325 - val_mae: 0.2077\n",
            "Epoch 73/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0311 - mae: 0.2037 - val_loss: 0.0324 - val_mae: 0.2073\n",
            "Epoch 74/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0324 - mae: 0.2050 - val_loss: 0.0322 - val_mae: 0.2067\n",
            "Epoch 75/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0329 - mae: 0.2084 - val_loss: 0.0321 - val_mae: 0.2061\n",
            "Epoch 76/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0303 - mae: 0.1989 - val_loss: 0.0320 - val_mae: 0.2058\n",
            "Epoch 77/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0323 - mae: 0.2084 - val_loss: 0.0318 - val_mae: 0.2051\n",
            "Epoch 78/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0312 - mae: 0.2041 - val_loss: 0.0317 - val_mae: 0.2047\n",
            "Epoch 79/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0308 - mae: 0.2008 - val_loss: 0.0316 - val_mae: 0.2042\n",
            "Epoch 80/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0288 - mae: 0.1944 - val_loss: 0.0315 - val_mae: 0.2038\n",
            "Epoch 81/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0291 - mae: 0.1959 - val_loss: 0.0313 - val_mae: 0.2034\n",
            "Epoch 82/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0297 - mae: 0.1975 - val_loss: 0.0312 - val_mae: 0.2030\n",
            "Epoch 83/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0308 - mae: 0.2008 - val_loss: 0.0311 - val_mae: 0.2026\n",
            "Epoch 84/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0302 - mae: 0.1979 - val_loss: 0.0310 - val_mae: 0.2023\n",
            "Epoch 85/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0293 - mae: 0.1957 - val_loss: 0.0309 - val_mae: 0.2018\n",
            "Epoch 86/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0314 - mae: 0.2013 - val_loss: 0.0308 - val_mae: 0.2015\n",
            "Epoch 87/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0295 - mae: 0.1968 - val_loss: 0.0307 - val_mae: 0.2011\n",
            "Epoch 88/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0284 - mae: 0.1945 - val_loss: 0.0305 - val_mae: 0.2006\n",
            "Epoch 89/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0280 - mae: 0.1908 - val_loss: 0.0304 - val_mae: 0.2003\n",
            "Epoch 90/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0283 - mae: 0.1945 - val_loss: 0.0303 - val_mae: 0.2000\n",
            "Epoch 91/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0279 - mae: 0.1911 - val_loss: 0.0302 - val_mae: 0.1995\n",
            "Epoch 92/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0302 - mae: 0.1985 - val_loss: 0.0301 - val_mae: 0.1992\n",
            "Epoch 93/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0288 - mae: 0.1930 - val_loss: 0.0300 - val_mae: 0.1987\n",
            "Epoch 94/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0287 - mae: 0.1937 - val_loss: 0.0299 - val_mae: 0.1985\n",
            "Epoch 95/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0286 - mae: 0.1941 - val_loss: 0.0299 - val_mae: 0.1981\n",
            "Epoch 96/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0271 - mae: 0.1883 - val_loss: 0.0298 - val_mae: 0.1978\n",
            "Epoch 97/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0274 - mae: 0.1897 - val_loss: 0.0297 - val_mae: 0.1974\n",
            "Epoch 98/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0286 - mae: 0.1936 - val_loss: 0.0296 - val_mae: 0.1971\n",
            "Epoch 99/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0270 - mae: 0.1879 - val_loss: 0.0295 - val_mae: 0.1968\n",
            "Epoch 100/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0294 - mae: 0.1951 - val_loss: 0.0294 - val_mae: 0.1964\n",
            "Epoch 101/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0287 - mae: 0.1961 - val_loss: 0.0293 - val_mae: 0.1963\n",
            "Epoch 102/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0274 - mae: 0.1896 - val_loss: 0.0292 - val_mae: 0.1960\n",
            "Epoch 103/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0274 - mae: 0.1912 - val_loss: 0.0291 - val_mae: 0.1958\n",
            "Epoch 104/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0274 - mae: 0.1912 - val_loss: 0.0291 - val_mae: 0.1956\n",
            "Epoch 105/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0274 - mae: 0.1915 - val_loss: 0.0290 - val_mae: 0.1952\n",
            "Epoch 106/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0265 - mae: 0.1851 - val_loss: 0.0289 - val_mae: 0.1947\n",
            "Epoch 107/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0281 - mae: 0.1927 - val_loss: 0.0288 - val_mae: 0.1945\n",
            "Epoch 108/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0272 - mae: 0.1885 - val_loss: 0.0288 - val_mae: 0.1942\n",
            "Epoch 109/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0259 - mae: 0.1825 - val_loss: 0.0287 - val_mae: 0.1940\n",
            "Epoch 110/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0265 - mae: 0.1871 - val_loss: 0.0286 - val_mae: 0.1936\n",
            "Epoch 111/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0277 - mae: 0.1887 - val_loss: 0.0285 - val_mae: 0.1932\n",
            "Epoch 112/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0262 - mae: 0.1859 - val_loss: 0.0285 - val_mae: 0.1931\n",
            "Epoch 113/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0261 - mae: 0.1844 - val_loss: 0.0284 - val_mae: 0.1929\n",
            "Epoch 114/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0273 - mae: 0.1899 - val_loss: 0.0283 - val_mae: 0.1925\n",
            "Epoch 115/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0267 - mae: 0.1880 - val_loss: 0.0283 - val_mae: 0.1922\n",
            "Epoch 116/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0263 - mae: 0.1859 - val_loss: 0.0282 - val_mae: 0.1920\n",
            "Epoch 117/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0268 - mae: 0.1881 - val_loss: 0.0281 - val_mae: 0.1915\n",
            "Epoch 118/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0265 - mae: 0.1860 - val_loss: 0.0281 - val_mae: 0.1914\n",
            "Epoch 119/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0265 - mae: 0.1851 - val_loss: 0.0280 - val_mae: 0.1913\n",
            "Epoch 120/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0264 - mae: 0.1871 - val_loss: 0.0279 - val_mae: 0.1909\n",
            "Epoch 121/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0256 - mae: 0.1803 - val_loss: 0.0279 - val_mae: 0.1907\n",
            "Epoch 122/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0244 - mae: 0.1794 - val_loss: 0.0278 - val_mae: 0.1903\n",
            "Epoch 123/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0252 - mae: 0.1827 - val_loss: 0.0278 - val_mae: 0.1902\n",
            "Epoch 124/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0270 - mae: 0.1880 - val_loss: 0.0277 - val_mae: 0.1901\n",
            "Epoch 125/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0260 - mae: 0.1839 - val_loss: 0.0276 - val_mae: 0.1898\n",
            "Epoch 126/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0250 - mae: 0.1807 - val_loss: 0.0276 - val_mae: 0.1895\n",
            "Epoch 127/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0234 - mae: 0.1746 - val_loss: 0.0275 - val_mae: 0.1893\n",
            "Epoch 128/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0257 - mae: 0.1821 - val_loss: 0.0275 - val_mae: 0.1891\n",
            "Epoch 129/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0261 - mae: 0.1843 - val_loss: 0.0274 - val_mae: 0.1887\n",
            "Epoch 130/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0247 - mae: 0.1817 - val_loss: 0.0274 - val_mae: 0.1887\n",
            "Epoch 131/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0249 - mae: 0.1821 - val_loss: 0.0273 - val_mae: 0.1883\n",
            "Epoch 132/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0241 - mae: 0.1782 - val_loss: 0.0273 - val_mae: 0.1883\n",
            "Epoch 133/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0237 - mae: 0.1768 - val_loss: 0.0272 - val_mae: 0.1881\n",
            "Epoch 134/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0254 - mae: 0.1826 - val_loss: 0.0272 - val_mae: 0.1878\n",
            "Epoch 135/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0252 - mae: 0.1804 - val_loss: 0.0272 - val_mae: 0.1876\n",
            "Epoch 136/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0245 - mae: 0.1791 - val_loss: 0.0271 - val_mae: 0.1875\n",
            "Epoch 137/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0258 - mae: 0.1837 - val_loss: 0.0271 - val_mae: 0.1873\n",
            "Epoch 138/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0243 - mae: 0.1794 - val_loss: 0.0270 - val_mae: 0.1872\n",
            "Epoch 139/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0240 - mae: 0.1782 - val_loss: 0.0270 - val_mae: 0.1870\n",
            "Epoch 140/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0244 - mae: 0.1795 - val_loss: 0.0269 - val_mae: 0.1869\n",
            "Epoch 141/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0236 - mae: 0.1766 - val_loss: 0.0269 - val_mae: 0.1867\n",
            "Epoch 142/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0237 - mae: 0.1769 - val_loss: 0.0268 - val_mae: 0.1864\n",
            "Epoch 143/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0238 - mae: 0.1781 - val_loss: 0.0268 - val_mae: 0.1862\n",
            "Epoch 144/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0240 - mae: 0.1771 - val_loss: 0.0267 - val_mae: 0.1860\n",
            "Epoch 145/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0239 - mae: 0.1768 - val_loss: 0.0267 - val_mae: 0.1859\n",
            "Epoch 146/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0245 - mae: 0.1781 - val_loss: 0.0267 - val_mae: 0.1858\n",
            "Epoch 147/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0242 - mae: 0.1789 - val_loss: 0.0266 - val_mae: 0.1857\n",
            "Epoch 148/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0238 - mae: 0.1775 - val_loss: 0.0266 - val_mae: 0.1855\n",
            "Epoch 149/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0236 - mae: 0.1747 - val_loss: 0.0265 - val_mae: 0.1853\n",
            "Epoch 150/150\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0241 - mae: 0.1779 - val_loss: 0.0265 - val_mae: 0.1853\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "def create_model_2():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adagrad', loss=Huber(), metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_2 = scaler.fit_transform(X_train_2)\n",
        "X_test_2 = scaler.transform(X_test_2)\n",
        "\n",
        "model_2 = create_model_2()\n",
        "history_2 = model_2.fit(X_train_2, y_train_2, epochs=150, batch_size=15, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_2, test_mae_2 = model_2.evaluate(X_test_2, y_test_2)\n",
        "\n",
        "print(f\"Modelo 2 - Test Loss (MSE): {test_loss_2}, Test MAE: {test_mae_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApCwpE7B_uRZ",
        "outputId": "0e3d8fb8-ffec-44b5-e36d-4cf0a5f8382f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0327 - mae: 0.1973\n",
            "Modelo 2 - Test Loss (MSE): 0.03059302270412445, Test MAE: 0.19421742856502533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_2 = model_2.predict(X_test_2)\n",
        "\n",
        "for i in range(len(predictions_2)):\n",
        "    print(f\"Predicted GPA: {predictions_2[i][0]:.4f}, Actual GPA: {y_test_2.iloc[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfkqgtrPBv5B",
        "outputId": "cdefcbd7-d3bd-45ae-c56f-2efb1495d373"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Predicted GPA: 1.8310, Actual GPA: 1.4277\n",
            "Predicted GPA: 3.2178, Actual GPA: 3.1174\n",
            "Predicted GPA: 1.8244, Actual GPA: 2.0378\n",
            "Predicted GPA: 3.5764, Actual GPA: 3.5485\n",
            "Predicted GPA: 0.4487, Actual GPA: 0.2490\n",
            "Predicted GPA: 2.7354, Actual GPA: 2.6277\n",
            "Predicted GPA: 1.4754, Actual GPA: 2.0574\n",
            "Predicted GPA: 2.1269, Actual GPA: 2.2483\n",
            "Predicted GPA: 1.9950, Actual GPA: 2.1947\n",
            "Predicted GPA: 0.8114, Actual GPA: 0.7582\n",
            "Predicted GPA: 2.2869, Actual GPA: 2.3709\n",
            "Predicted GPA: 0.5023, Actual GPA: 0.7664\n",
            "Predicted GPA: 3.0583, Actual GPA: 2.9527\n",
            "Predicted GPA: 2.5903, Actual GPA: 2.3433\n",
            "Predicted GPA: 2.7482, Actual GPA: 2.7718\n",
            "Predicted GPA: 0.3936, Actual GPA: 0.2879\n",
            "Predicted GPA: 1.1959, Actual GPA: 1.0183\n",
            "Predicted GPA: 1.2110, Actual GPA: 1.6294\n",
            "Predicted GPA: 1.8944, Actual GPA: 2.0744\n",
            "Predicted GPA: 2.6610, Actual GPA: 2.4238\n",
            "Predicted GPA: 2.4083, Actual GPA: 1.7562\n",
            "Predicted GPA: 1.4412, Actual GPA: 1.5663\n",
            "Predicted GPA: 1.7110, Actual GPA: 1.7062\n",
            "Predicted GPA: 3.2365, Actual GPA: 3.1614\n",
            "Predicted GPA: 1.8831, Actual GPA: 1.7334\n",
            "Predicted GPA: 0.5893, Actual GPA: 0.8420\n",
            "Predicted GPA: 1.8742, Actual GPA: 1.3792\n",
            "Predicted GPA: 2.3320, Actual GPA: 3.0270\n",
            "Predicted GPA: 2.1525, Actual GPA: 2.1920\n",
            "Predicted GPA: 1.6768, Actual GPA: 2.3158\n",
            "Predicted GPA: 2.1362, Actual GPA: 2.0681\n",
            "Predicted GPA: 0.6466, Actual GPA: 0.8691\n",
            "Predicted GPA: 2.9928, Actual GPA: 2.9001\n",
            "Predicted GPA: 3.1684, Actual GPA: 3.4686\n",
            "Predicted GPA: 1.5433, Actual GPA: 1.5674\n",
            "Predicted GPA: 1.8148, Actual GPA: 1.7947\n",
            "Predicted GPA: 3.2863, Actual GPA: 3.1813\n",
            "Predicted GPA: 3.1082, Actual GPA: 2.8974\n",
            "Predicted GPA: 3.2162, Actual GPA: 3.2449\n",
            "Predicted GPA: 0.5835, Actual GPA: 0.3578\n",
            "Predicted GPA: 2.6784, Actual GPA: 2.6524\n",
            "Predicted GPA: 3.5394, Actual GPA: 3.6810\n",
            "Predicted GPA: 1.3963, Actual GPA: 1.0364\n",
            "Predicted GPA: 2.1223, Actual GPA: 2.0172\n",
            "Predicted GPA: 0.9974, Actual GPA: 0.9634\n",
            "Predicted GPA: 2.5878, Actual GPA: 2.2395\n",
            "Predicted GPA: 2.8979, Actual GPA: 2.7360\n",
            "Predicted GPA: 0.9858, Actual GPA: 1.3619\n",
            "Predicted GPA: 2.8132, Actual GPA: 2.7030\n",
            "Predicted GPA: 1.9049, Actual GPA: 1.4419\n",
            "Predicted GPA: 3.0409, Actual GPA: 3.2195\n",
            "Predicted GPA: 3.1651, Actual GPA: 3.3391\n",
            "Predicted GPA: 1.6665, Actual GPA: 1.5562\n",
            "Predicted GPA: 1.3606, Actual GPA: 1.3424\n",
            "Predicted GPA: 1.9658, Actual GPA: 1.7562\n",
            "Predicted GPA: 3.3787, Actual GPA: 3.4218\n",
            "Predicted GPA: 2.7391, Actual GPA: 2.3170\n",
            "Predicted GPA: 3.4121, Actual GPA: 3.2866\n",
            "Predicted GPA: 0.7939, Actual GPA: 0.6847\n",
            "Predicted GPA: 2.1832, Actual GPA: 2.1371\n",
            "Predicted GPA: 1.9322, Actual GPA: 1.6885\n",
            "Predicted GPA: 2.0257, Actual GPA: 2.1962\n",
            "Predicted GPA: 2.3114, Actual GPA: 2.4778\n",
            "Predicted GPA: 1.5963, Actual GPA: 1.1979\n",
            "Predicted GPA: 0.9773, Actual GPA: 0.9882\n",
            "Predicted GPA: 2.4138, Actual GPA: 1.8589\n",
            "Predicted GPA: 3.4250, Actual GPA: 3.0407\n",
            "Predicted GPA: 2.8661, Actual GPA: 2.3741\n",
            "Predicted GPA: 1.2076, Actual GPA: 1.2213\n",
            "Predicted GPA: 3.5218, Actual GPA: 3.2742\n",
            "Predicted GPA: 3.5043, Actual GPA: 3.5452\n",
            "Predicted GPA: 0.7051, Actual GPA: 1.1019\n",
            "Predicted GPA: 3.0324, Actual GPA: 2.9688\n",
            "Predicted GPA: 3.0720, Actual GPA: 2.5762\n",
            "Predicted GPA: 1.0008, Actual GPA: 0.4541\n",
            "Predicted GPA: 2.8652, Actual GPA: 2.6742\n",
            "Predicted GPA: 1.6862, Actual GPA: 2.0600\n",
            "Predicted GPA: 1.7721, Actual GPA: 1.9831\n",
            "Predicted GPA: 0.2533, Actual GPA: 0.2124\n",
            "Predicted GPA: 1.3482, Actual GPA: 1.2794\n",
            "Predicted GPA: 1.9927, Actual GPA: 2.0297\n",
            "Predicted GPA: 3.2009, Actual GPA: 3.1892\n",
            "Predicted GPA: 2.2569, Actual GPA: 2.2565\n",
            "Predicted GPA: 1.4563, Actual GPA: 1.6364\n",
            "Predicted GPA: 1.3287, Actual GPA: 1.4016\n",
            "Predicted GPA: 0.5592, Actual GPA: 0.4436\n",
            "Predicted GPA: 4.0534, Actual GPA: 4.0000\n",
            "Predicted GPA: 1.1460, Actual GPA: 1.2712\n",
            "Predicted GPA: 1.4723, Actual GPA: 1.3026\n",
            "Predicted GPA: 1.6177, Actual GPA: 1.5970\n",
            "Predicted GPA: 2.5295, Actual GPA: 2.5354\n",
            "Predicted GPA: 1.4302, Actual GPA: 1.3829\n",
            "Predicted GPA: 1.0052, Actual GPA: 1.0385\n",
            "Predicted GPA: 0.8392, Actual GPA: 0.1307\n",
            "Predicted GPA: 1.6529, Actual GPA: 1.2738\n",
            "Predicted GPA: 1.1864, Actual GPA: 1.2715\n",
            "Predicted GPA: 3.4543, Actual GPA: 3.3251\n",
            "Predicted GPA: 1.3219, Actual GPA: 1.0707\n",
            "Predicted GPA: 1.5978, Actual GPA: 1.5743\n",
            "Predicted GPA: 1.4591, Actual GPA: 1.6337\n",
            "Predicted GPA: 0.4748, Actual GPA: 0.0000\n",
            "Predicted GPA: 2.1258, Actual GPA: 2.1221\n",
            "Predicted GPA: 1.0713, Actual GPA: 1.2912\n",
            "Predicted GPA: 1.5225, Actual GPA: 1.4547\n",
            "Predicted GPA: 1.0729, Actual GPA: 0.7743\n",
            "Predicted GPA: 2.1944, Actual GPA: 2.2931\n",
            "Predicted GPA: 2.5727, Actual GPA: 2.6828\n",
            "Predicted GPA: 1.1669, Actual GPA: 1.3155\n",
            "Predicted GPA: 1.3461, Actual GPA: 1.4694\n",
            "Predicted GPA: 0.6662, Actual GPA: 0.6132\n",
            "Predicted GPA: 1.6336, Actual GPA: 1.8820\n",
            "Predicted GPA: 1.9974, Actual GPA: 2.2689\n",
            "Predicted GPA: 3.2324, Actual GPA: 3.5452\n",
            "Predicted GPA: 0.6273, Actual GPA: 0.4258\n",
            "Predicted GPA: 0.6194, Actual GPA: 0.5404\n",
            "Predicted GPA: 1.2805, Actual GPA: 1.1111\n",
            "Predicted GPA: 2.0873, Actual GPA: 2.0354\n",
            "Predicted GPA: 2.9628, Actual GPA: 2.9955\n",
            "Predicted GPA: 0.7437, Actual GPA: 1.2179\n",
            "Predicted GPA: 1.9966, Actual GPA: 2.1270\n",
            "Predicted GPA: 0.7406, Actual GPA: 0.5298\n",
            "Predicted GPA: 3.1329, Actual GPA: 3.0169\n",
            "Predicted GPA: 3.4286, Actual GPA: 3.5921\n",
            "Predicted GPA: 1.2137, Actual GPA: 1.0583\n",
            "Predicted GPA: 1.9919, Actual GPA: 1.8426\n",
            "Predicted GPA: 1.0907, Actual GPA: 1.2546\n",
            "Predicted GPA: 2.4931, Actual GPA: 2.7949\n",
            "Predicted GPA: 0.9154, Actual GPA: 1.3479\n",
            "Predicted GPA: 1.9164, Actual GPA: 2.2270\n",
            "Predicted GPA: 2.7335, Actual GPA: 2.8135\n",
            "Predicted GPA: 1.0003, Actual GPA: 1.1056\n",
            "Predicted GPA: 4.4490, Actual GPA: 4.0000\n",
            "Predicted GPA: 2.7440, Actual GPA: 2.8982\n",
            "Predicted GPA: 0.4107, Actual GPA: 0.4240\n",
            "Predicted GPA: 0.7602, Actual GPA: 0.6915\n",
            "Predicted GPA: 0.6509, Actual GPA: 0.9300\n",
            "Predicted GPA: 2.0894, Actual GPA: 2.3308\n",
            "Predicted GPA: 0.2339, Actual GPA: 0.3376\n",
            "Predicted GPA: 0.3647, Actual GPA: 0.5022\n",
            "Predicted GPA: 1.1872, Actual GPA: 0.9893\n",
            "Predicted GPA: 2.4928, Actual GPA: 2.7459\n",
            "Predicted GPA: 1.6872, Actual GPA: 1.8042\n",
            "Predicted GPA: 3.0999, Actual GPA: 3.3433\n",
            "Predicted GPA: 2.0161, Actual GPA: 1.5731\n",
            "Predicted GPA: 2.8868, Actual GPA: 2.9640\n",
            "Predicted GPA: 1.2883, Actual GPA: 1.1936\n",
            "Predicted GPA: 1.5941, Actual GPA: 1.3493\n",
            "Predicted GPA: 2.1836, Actual GPA: 2.5202\n",
            "Predicted GPA: 3.3062, Actual GPA: 3.2538\n",
            "Predicted GPA: 1.4745, Actual GPA: 1.4634\n",
            "Predicted GPA: 3.2404, Actual GPA: 3.1620\n",
            "Predicted GPA: 0.7018, Actual GPA: 0.7696\n",
            "Predicted GPA: 3.1621, Actual GPA: 3.5037\n",
            "Predicted GPA: 1.5478, Actual GPA: 1.2688\n",
            "Predicted GPA: 0.9732, Actual GPA: 1.1708\n",
            "Predicted GPA: 0.5373, Actual GPA: 0.4713\n",
            "Predicted GPA: 1.3483, Actual GPA: 1.3979\n",
            "Predicted GPA: 0.9552, Actual GPA: 0.9690\n",
            "Predicted GPA: 1.2552, Actual GPA: 1.5113\n",
            "Predicted GPA: 0.6585, Actual GPA: 0.5242\n",
            "Predicted GPA: 1.3802, Actual GPA: 1.4914\n",
            "Predicted GPA: 1.1163, Actual GPA: 1.5194\n",
            "Predicted GPA: 0.7733, Actual GPA: 0.7133\n",
            "Predicted GPA: 0.6942, Actual GPA: 0.4006\n",
            "Predicted GPA: 0.4821, Actual GPA: 0.5814\n",
            "Predicted GPA: 0.7003, Actual GPA: 0.4051\n",
            "Predicted GPA: 0.6222, Actual GPA: 1.1544\n",
            "Predicted GPA: 0.9981, Actual GPA: 1.1502\n",
            "Predicted GPA: 2.3717, Actual GPA: 2.4579\n",
            "Predicted GPA: 3.4154, Actual GPA: 3.4555\n",
            "Predicted GPA: 1.5745, Actual GPA: 1.4414\n",
            "Predicted GPA: 3.5836, Actual GPA: 3.2880\n",
            "Predicted GPA: 2.8617, Actual GPA: 3.0439\n",
            "Predicted GPA: 1.3541, Actual GPA: 0.1898\n",
            "Predicted GPA: 1.2059, Actual GPA: 1.1658\n",
            "Predicted GPA: 2.6031, Actual GPA: 2.5654\n",
            "Predicted GPA: 2.3856, Actual GPA: 2.2347\n",
            "Predicted GPA: 1.1162, Actual GPA: 1.2587\n",
            "Predicted GPA: 2.2209, Actual GPA: 2.1167\n",
            "Predicted GPA: 1.5514, Actual GPA: 1.6747\n",
            "Predicted GPA: 2.9941, Actual GPA: 2.8211\n",
            "Predicted GPA: 1.8382, Actual GPA: 1.9961\n",
            "Predicted GPA: 0.7086, Actual GPA: 0.8091\n",
            "Predicted GPA: 3.0849, Actual GPA: 2.5466\n",
            "Predicted GPA: 2.9418, Actual GPA: 2.7350\n",
            "Predicted GPA: 2.9063, Actual GPA: 2.7536\n",
            "Predicted GPA: 3.3563, Actual GPA: 3.2839\n",
            "Predicted GPA: 1.4694, Actual GPA: 1.2878\n",
            "Predicted GPA: 0.3187, Actual GPA: 0.3107\n",
            "Predicted GPA: 1.8882, Actual GPA: 1.8315\n",
            "Predicted GPA: 1.1917, Actual GPA: 0.9116\n",
            "Predicted GPA: 1.9790, Actual GPA: 2.1097\n",
            "Predicted GPA: 1.1362, Actual GPA: 1.0445\n",
            "Predicted GPA: 0.8334, Actual GPA: 0.9655\n",
            "Predicted GPA: 1.9461, Actual GPA: 1.5504\n",
            "Predicted GPA: 1.4439, Actual GPA: 1.2238\n",
            "Predicted GPA: 1.6287, Actual GPA: 1.8934\n",
            "Predicted GPA: 1.9359, Actual GPA: 2.2954\n",
            "Predicted GPA: 2.8506, Actual GPA: 2.8393\n",
            "Predicted GPA: 2.4900, Actual GPA: 2.2518\n",
            "Predicted GPA: 1.3530, Actual GPA: 1.4053\n",
            "Predicted GPA: 2.8957, Actual GPA: 3.0321\n",
            "Predicted GPA: 0.7883, Actual GPA: 0.5694\n",
            "Predicted GPA: 2.3351, Actual GPA: 2.2201\n",
            "Predicted GPA: 3.0950, Actual GPA: 2.7374\n",
            "Predicted GPA: 1.8854, Actual GPA: 2.3408\n",
            "Predicted GPA: 0.4187, Actual GPA: 0.5602\n",
            "Predicted GPA: 3.0589, Actual GPA: 2.9139\n",
            "Predicted GPA: 1.9985, Actual GPA: 2.2595\n",
            "Predicted GPA: 1.4211, Actual GPA: 1.7420\n",
            "Predicted GPA: 1.5685, Actual GPA: 1.2659\n",
            "Predicted GPA: 0.8974, Actual GPA: 1.2987\n",
            "Predicted GPA: 2.1563, Actual GPA: 1.9138\n",
            "Predicted GPA: 2.5841, Actual GPA: 2.7808\n",
            "Predicted GPA: 2.3873, Actual GPA: 2.5535\n",
            "Predicted GPA: 1.0785, Actual GPA: 0.8923\n",
            "Predicted GPA: 3.0619, Actual GPA: 2.6697\n",
            "Predicted GPA: 3.1103, Actual GPA: 3.0885\n",
            "Predicted GPA: 1.7330, Actual GPA: 1.5636\n",
            "Predicted GPA: 2.8230, Actual GPA: 2.8757\n",
            "Predicted GPA: 1.4256, Actual GPA: 1.1845\n",
            "Predicted GPA: 2.4034, Actual GPA: 2.5842\n",
            "Predicted GPA: 3.4026, Actual GPA: 3.8128\n",
            "Predicted GPA: 0.4412, Actual GPA: 0.6729\n",
            "Predicted GPA: 1.4330, Actual GPA: 1.9559\n",
            "Predicted GPA: 1.5764, Actual GPA: 1.7733\n",
            "Predicted GPA: 3.1532, Actual GPA: 3.0554\n",
            "Predicted GPA: 2.6621, Actual GPA: 2.7185\n",
            "Predicted GPA: 1.1872, Actual GPA: 1.2705\n",
            "Predicted GPA: 2.0551, Actual GPA: 1.9880\n",
            "Predicted GPA: 1.6021, Actual GPA: 1.7209\n",
            "Predicted GPA: 1.8850, Actual GPA: 2.1106\n",
            "Predicted GPA: 2.8150, Actual GPA: 2.8816\n",
            "Predicted GPA: 0.9730, Actual GPA: 0.9708\n",
            "Predicted GPA: 2.8962, Actual GPA: 2.9729\n",
            "Predicted GPA: 2.6795, Actual GPA: 2.5210\n",
            "Predicted GPA: 0.7151, Actual GPA: 0.5152\n",
            "Predicted GPA: 1.1418, Actual GPA: 1.0086\n",
            "Predicted GPA: 2.6040, Actual GPA: 2.7913\n",
            "Predicted GPA: 1.2197, Actual GPA: 0.5868\n",
            "Predicted GPA: 3.4846, Actual GPA: 3.6663\n",
            "Predicted GPA: 0.9449, Actual GPA: 0.9031\n",
            "Predicted GPA: 0.4364, Actual GPA: 0.7046\n",
            "Predicted GPA: 2.1568, Actual GPA: 2.3983\n",
            "Predicted GPA: 1.4074, Actual GPA: 1.8178\n",
            "Predicted GPA: 0.6936, Actual GPA: 0.7996\n",
            "Predicted GPA: 2.3968, Actual GPA: 2.1274\n",
            "Predicted GPA: 1.4022, Actual GPA: 1.1768\n",
            "Predicted GPA: 0.6984, Actual GPA: 0.4291\n",
            "Predicted GPA: 1.8015, Actual GPA: 2.0469\n",
            "Predicted GPA: 1.2425, Actual GPA: 1.0862\n",
            "Predicted GPA: 3.0914, Actual GPA: 2.9869\n",
            "Predicted GPA: 2.3464, Actual GPA: 2.4740\n",
            "Predicted GPA: 1.8701, Actual GPA: 1.5819\n",
            "Predicted GPA: 2.3809, Actual GPA: 2.3278\n",
            "Predicted GPA: 1.7998, Actual GPA: 1.5253\n",
            "Predicted GPA: 1.4207, Actual GPA: 1.6831\n",
            "Predicted GPA: 2.2229, Actual GPA: 2.4610\n",
            "Predicted GPA: 0.4817, Actual GPA: 0.0278\n",
            "Predicted GPA: 1.4504, Actual GPA: 1.5364\n",
            "Predicted GPA: 1.9394, Actual GPA: 2.0999\n",
            "Predicted GPA: 0.5123, Actual GPA: 0.3751\n",
            "Predicted GPA: 1.7287, Actual GPA: 1.2946\n",
            "Predicted GPA: 0.7994, Actual GPA: 0.7697\n",
            "Predicted GPA: 1.9012, Actual GPA: 1.8214\n",
            "Predicted GPA: 3.4349, Actual GPA: 3.4983\n",
            "Predicted GPA: 2.6360, Actual GPA: 2.3781\n",
            "Predicted GPA: 0.7470, Actual GPA: 0.5185\n",
            "Predicted GPA: 2.5377, Actual GPA: 2.7549\n",
            "Predicted GPA: 1.9880, Actual GPA: 2.2070\n",
            "Predicted GPA: 1.2781, Actual GPA: 1.1416\n",
            "Predicted GPA: 3.3960, Actual GPA: 3.1376\n",
            "Predicted GPA: 1.5233, Actual GPA: 1.3638\n",
            "Predicted GPA: 1.5215, Actual GPA: 1.6885\n",
            "Predicted GPA: 1.3238, Actual GPA: 1.3503\n",
            "Predicted GPA: 2.6355, Actual GPA: 2.0625\n",
            "Predicted GPA: 0.9518, Actual GPA: 1.0226\n",
            "Predicted GPA: 2.2854, Actual GPA: 1.7923\n",
            "Predicted GPA: 0.4995, Actual GPA: 0.0000\n",
            "Predicted GPA: 2.6088, Actual GPA: 2.8729\n",
            "Predicted GPA: 3.1861, Actual GPA: 3.0887\n",
            "Predicted GPA: 2.5325, Actual GPA: 2.9375\n",
            "Predicted GPA: 1.7798, Actual GPA: 1.9901\n",
            "Predicted GPA: 1.4260, Actual GPA: 1.5165\n",
            "Predicted GPA: 3.0676, Actual GPA: 3.3009\n",
            "Predicted GPA: 2.1732, Actual GPA: 2.1727\n",
            "Predicted GPA: 1.1045, Actual GPA: 1.1194\n",
            "Predicted GPA: 1.3730, Actual GPA: 1.5718\n",
            "Predicted GPA: 0.7236, Actual GPA: 0.4959\n",
            "Predicted GPA: 0.5096, Actual GPA: 0.1550\n",
            "Predicted GPA: 3.7130, Actual GPA: 3.8650\n",
            "Predicted GPA: 1.9507, Actual GPA: 1.8044\n",
            "Predicted GPA: 2.3989, Actual GPA: 2.7842\n",
            "Predicted GPA: 1.1128, Actual GPA: 1.2059\n",
            "Predicted GPA: 2.2786, Actual GPA: 2.8236\n",
            "Predicted GPA: 1.1510, Actual GPA: 0.8952\n",
            "Predicted GPA: 2.9385, Actual GPA: 3.1123\n",
            "Predicted GPA: 1.5176, Actual GPA: 1.8154\n",
            "Predicted GPA: 0.2440, Actual GPA: 0.1006\n",
            "Predicted GPA: 2.8815, Actual GPA: 3.0655\n",
            "Predicted GPA: 2.9364, Actual GPA: 2.5539\n",
            "Predicted GPA: 2.1661, Actual GPA: 1.9864\n",
            "Predicted GPA: 0.9722, Actual GPA: 0.9941\n",
            "Predicted GPA: 3.2579, Actual GPA: 3.3501\n",
            "Predicted GPA: 2.1899, Actual GPA: 2.3117\n",
            "Predicted GPA: 2.6834, Actual GPA: 2.5042\n",
            "Predicted GPA: 0.5040, Actual GPA: 0.7635\n",
            "Predicted GPA: 2.2697, Actual GPA: 2.7122\n",
            "Predicted GPA: 0.9303, Actual GPA: 0.9253\n",
            "Predicted GPA: 1.8881, Actual GPA: 1.8498\n",
            "Predicted GPA: 1.2592, Actual GPA: 1.3971\n",
            "Predicted GPA: 1.7528, Actual GPA: 1.8890\n",
            "Predicted GPA: 0.8631, Actual GPA: 0.9000\n",
            "Predicted GPA: 1.1575, Actual GPA: 1.0277\n",
            "Predicted GPA: 0.9227, Actual GPA: 0.7017\n",
            "Predicted GPA: 1.2868, Actual GPA: 1.4120\n",
            "Predicted GPA: 2.9028, Actual GPA: 2.9774\n",
            "Predicted GPA: 2.3595, Actual GPA: 2.2104\n",
            "Predicted GPA: 2.4927, Actual GPA: 2.4387\n",
            "Predicted GPA: 1.6799, Actual GPA: 1.9213\n",
            "Predicted GPA: 2.0400, Actual GPA: 1.7340\n",
            "Predicted GPA: 1.5002, Actual GPA: 1.5893\n",
            "Predicted GPA: 1.7930, Actual GPA: 1.5034\n",
            "Predicted GPA: 2.0923, Actual GPA: 2.2220\n",
            "Predicted GPA: 3.0641, Actual GPA: 3.1062\n",
            "Predicted GPA: 1.4415, Actual GPA: 1.5421\n",
            "Predicted GPA: 0.8193, Actual GPA: 1.2523\n",
            "Predicted GPA: 1.3602, Actual GPA: 1.7105\n",
            "Predicted GPA: 2.7716, Actual GPA: 2.6936\n",
            "Predicted GPA: 1.2419, Actual GPA: 0.9657\n",
            "Predicted GPA: 1.6781, Actual GPA: 1.8683\n",
            "Predicted GPA: 3.0677, Actual GPA: 2.8069\n",
            "Predicted GPA: 2.1345, Actual GPA: 2.3428\n",
            "Predicted GPA: 2.5254, Actual GPA: 2.5837\n",
            "Predicted GPA: 3.2290, Actual GPA: 3.2486\n",
            "Predicted GPA: 2.6336, Actual GPA: 2.5845\n",
            "Predicted GPA: 2.4786, Actual GPA: 2.5876\n",
            "Predicted GPA: 1.3831, Actual GPA: 1.5660\n",
            "Predicted GPA: 0.6445, Actual GPA: 0.8670\n",
            "Predicted GPA: 2.5816, Actual GPA: 1.8476\n",
            "Predicted GPA: 1.2747, Actual GPA: 1.1221\n",
            "Predicted GPA: 1.6787, Actual GPA: 1.7182\n",
            "Predicted GPA: 1.5110, Actual GPA: 1.1645\n",
            "Predicted GPA: 2.0470, Actual GPA: 1.6833\n",
            "Predicted GPA: 1.9206, Actual GPA: 2.0186\n",
            "Predicted GPA: 2.4908, Actual GPA: 2.5125\n",
            "Predicted GPA: 2.9429, Actual GPA: 3.1690\n",
            "Predicted GPA: 3.2437, Actual GPA: 3.0526\n",
            "Predicted GPA: 1.0482, Actual GPA: 1.1529\n",
            "Predicted GPA: 3.3393, Actual GPA: 3.2583\n",
            "Predicted GPA: 3.2988, Actual GPA: 3.3637\n",
            "Predicted GPA: 1.9983, Actual GPA: 2.4041\n",
            "Predicted GPA: 1.9754, Actual GPA: 2.1377\n",
            "Predicted GPA: 2.4385, Actual GPA: 2.4276\n",
            "Predicted GPA: 1.9669, Actual GPA: 1.8100\n",
            "Predicted GPA: 1.9031, Actual GPA: 1.7560\n",
            "Predicted GPA: 2.3743, Actual GPA: 2.0531\n",
            "Predicted GPA: 1.2188, Actual GPA: 1.6015\n",
            "Predicted GPA: 0.9381, Actual GPA: 0.4696\n",
            "Predicted GPA: 2.3290, Actual GPA: 2.0046\n",
            "Predicted GPA: 2.6981, Actual GPA: 2.2888\n",
            "Predicted GPA: 3.1892, Actual GPA: 3.0917\n",
            "Predicted GPA: 2.2946, Actual GPA: 2.1172\n",
            "Predicted GPA: 1.0928, Actual GPA: 1.3476\n",
            "Predicted GPA: 1.6392, Actual GPA: 1.5304\n",
            "Predicted GPA: 2.7988, Actual GPA: 2.7344\n",
            "Predicted GPA: 3.7031, Actual GPA: 3.8302\n",
            "Predicted GPA: 2.6216, Actual GPA: 2.6394\n",
            "Predicted GPA: 2.4432, Actual GPA: 2.4714\n",
            "Predicted GPA: 1.9203, Actual GPA: 2.2286\n",
            "Predicted GPA: 0.5370, Actual GPA: 0.6566\n",
            "Predicted GPA: 1.8448, Actual GPA: 2.2598\n",
            "Predicted GPA: 2.4184, Actual GPA: 2.3328\n",
            "Predicted GPA: 2.6541, Actual GPA: 3.0235\n",
            "Predicted GPA: 2.3939, Actual GPA: 2.9647\n",
            "Predicted GPA: 0.9541, Actual GPA: 0.9836\n",
            "Predicted GPA: 1.3819, Actual GPA: 1.4655\n",
            "Predicted GPA: 2.0719, Actual GPA: 2.2689\n",
            "Predicted GPA: 3.5508, Actual GPA: 3.5729\n",
            "Predicted GPA: 0.9718, Actual GPA: 0.6823\n",
            "Predicted GPA: 1.9470, Actual GPA: 1.9847\n",
            "Predicted GPA: 2.2168, Actual GPA: 2.5197\n",
            "Predicted GPA: 0.6901, Actual GPA: 0.5496\n",
            "Predicted GPA: 2.9547, Actual GPA: 2.9869\n",
            "Predicted GPA: 2.6161, Actual GPA: 2.6405\n",
            "Predicted GPA: 1.6649, Actual GPA: 1.4773\n",
            "Predicted GPA: 2.2966, Actual GPA: 1.9396\n",
            "Predicted GPA: 1.7810, Actual GPA: 1.8298\n",
            "Predicted GPA: 2.6720, Actual GPA: 2.6696\n",
            "Predicted GPA: 1.4540, Actual GPA: 1.5961\n",
            "Predicted GPA: 2.4724, Actual GPA: 2.2790\n",
            "Predicted GPA: 1.1027, Actual GPA: 1.1378\n",
            "Predicted GPA: 1.0610, Actual GPA: 0.6768\n",
            "Predicted GPA: 2.0617, Actual GPA: 2.5040\n",
            "Predicted GPA: 1.9808, Actual GPA: 2.1226\n",
            "Predicted GPA: 2.8650, Actual GPA: 2.9779\n",
            "Predicted GPA: 2.8718, Actual GPA: 2.8276\n",
            "Predicted GPA: 2.9698, Actual GPA: 2.9422\n",
            "Predicted GPA: 1.9295, Actual GPA: 2.3100\n",
            "Predicted GPA: 2.9091, Actual GPA: 2.7779\n",
            "Predicted GPA: 2.1970, Actual GPA: 2.5244\n",
            "Predicted GPA: 2.1574, Actual GPA: 2.1353\n",
            "Predicted GPA: 2.4483, Actual GPA: 2.2242\n",
            "Predicted GPA: 2.9482, Actual GPA: 3.2389\n",
            "Predicted GPA: 2.0109, Actual GPA: 2.2303\n",
            "Predicted GPA: 2.6343, Actual GPA: 3.0608\n",
            "Predicted GPA: 0.8074, Actual GPA: 1.3636\n",
            "Predicted GPA: 3.0274, Actual GPA: 3.6035\n",
            "Predicted GPA: 2.1221, Actual GPA: 2.1673\n",
            "Predicted GPA: 3.2078, Actual GPA: 3.3239\n",
            "Predicted GPA: 2.0626, Actual GPA: 2.0166\n",
            "Predicted GPA: 1.7149, Actual GPA: 1.3620\n",
            "Predicted GPA: 2.0636, Actual GPA: 2.1156\n",
            "Predicted GPA: 2.3441, Actual GPA: 2.4653\n",
            "Predicted GPA: 3.0664, Actual GPA: 3.0605\n",
            "Predicted GPA: 2.0805, Actual GPA: 1.9915\n",
            "Predicted GPA: 0.4129, Actual GPA: 0.2649\n",
            "Predicted GPA: 3.4604, Actual GPA: 3.6457\n",
            "Predicted GPA: 3.0910, Actual GPA: 2.5172\n",
            "Predicted GPA: 1.4106, Actual GPA: 1.5487\n",
            "Predicted GPA: 1.5214, Actual GPA: 1.5996\n",
            "Predicted GPA: 1.4927, Actual GPA: 1.5539\n",
            "Predicted GPA: 2.4411, Actual GPA: 2.5959\n",
            "Predicted GPA: 1.7052, Actual GPA: 1.8791\n",
            "Predicted GPA: 1.3910, Actual GPA: 1.5067\n",
            "Predicted GPA: 1.1306, Actual GPA: 1.1049\n",
            "Predicted GPA: 0.5210, Actual GPA: 0.3414\n",
            "Predicted GPA: 2.4807, Actual GPA: 2.4053\n",
            "Predicted GPA: 2.0387, Actual GPA: 1.6848\n",
            "Predicted GPA: 3.3192, Actual GPA: 2.9935\n",
            "Predicted GPA: 2.7829, Actual GPA: 3.4154\n",
            "Predicted GPA: 1.5010, Actual GPA: 1.7097\n",
            "Predicted GPA: 1.7261, Actual GPA: 1.5387\n",
            "Predicted GPA: 2.8606, Actual GPA: 2.9214\n",
            "Predicted GPA: 2.4993, Actual GPA: 2.6052\n",
            "Predicted GPA: 2.2677, Actual GPA: 2.4400\n",
            "Predicted GPA: 1.4521, Actual GPA: 1.4290\n",
            "Predicted GPA: 3.4505, Actual GPA: 3.3721\n",
            "Predicted GPA: 2.2096, Actual GPA: 2.1516\n",
            "Predicted GPA: 1.4219, Actual GPA: 1.5951\n",
            "Predicted GPA: 1.4167, Actual GPA: 1.4146\n",
            "Predicted GPA: 3.0068, Actual GPA: 3.1289\n",
            "Predicted GPA: 2.9427, Actual GPA: 2.5534\n",
            "Predicted GPA: 2.9817, Actual GPA: 3.1296\n",
            "Predicted GPA: 2.6980, Actual GPA: 2.9820\n",
            "Predicted GPA: 2.8514, Actual GPA: 2.9818\n",
            "Predicted GPA: 1.4478, Actual GPA: 1.2011\n",
            "Predicted GPA: 2.5357, Actual GPA: 2.7914\n",
            "Predicted GPA: 0.7320, Actual GPA: 0.1530\n",
            "Predicted GPA: 0.7583, Actual GPA: 0.6194\n",
            "Predicted GPA: 2.3358, Actual GPA: 2.2813\n",
            "Predicted GPA: 0.6189, Actual GPA: 0.4937\n",
            "Predicted GPA: 3.0431, Actual GPA: 2.9665\n",
            "Predicted GPA: 1.9044, Actual GPA: 1.9895\n",
            "Predicted GPA: 3.6310, Actual GPA: 3.5433\n",
            "Predicted GPA: 1.9137, Actual GPA: 1.9706\n",
            "Predicted GPA: 1.0293, Actual GPA: 1.3441\n",
            "Predicted GPA: 0.7858, Actual GPA: 1.0270\n",
            "Predicted GPA: 0.4663, Actual GPA: 0.4278\n",
            "Predicted GPA: 2.2131, Actual GPA: 1.9747\n",
            "Predicted GPA: 0.8228, Actual GPA: 1.2422\n",
            "Predicted GPA: 0.6999, Actual GPA: 0.3844\n",
            "Predicted GPA: 0.7345, Actual GPA: 0.4475\n",
            "Predicted GPA: 1.4086, Actual GPA: 1.7291\n",
            "Predicted GPA: 2.0767, Actual GPA: 2.1364\n",
            "Predicted GPA: 0.3360, Actual GPA: 0.2110\n",
            "Predicted GPA: 0.3295, Actual GPA: 0.3310\n",
            "Predicted GPA: 2.6247, Actual GPA: 2.8889\n",
            "Predicted GPA: 3.3593, Actual GPA: 3.2704\n",
            "Predicted GPA: 2.6359, Actual GPA: 2.5001\n",
            "Predicted GPA: 1.3685, Actual GPA: 1.2129\n",
            "Predicted GPA: 1.8830, Actual GPA: 2.0093\n",
            "Predicted GPA: 1.1738, Actual GPA: 1.5438\n",
            "Predicted GPA: 1.5008, Actual GPA: 1.4382\n",
            "Predicted GPA: 1.7117, Actual GPA: 1.5624\n",
            "Predicted GPA: 1.9198, Actual GPA: 2.1749\n",
            "Predicted GPA: 2.5487, Actual GPA: 2.3325\n",
            "Predicted GPA: 2.5155, Actual GPA: 2.7780\n",
            "Predicted GPA: 0.9610, Actual GPA: 0.8635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_data = {\n",
        "    'Modelo': ['Modelo 1', 'Modelo 2', 'Modelo 3'],\n",
        "    'Test Loss (MSE)': [test_loss, test_loss_1, test_loss_2],\n",
        "    'Test MAE': [test_mae, test_mae_1, test_mae_2]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"Comparación de Loss y MAE:\")\n",
        "print(comparison_df)\n",
        "\n",
        "predictions_flat = predictions.flatten()\n",
        "predictions_1_flat = predictions_1.flatten()\n",
        "predictions_2_flat = predictions_2.flatten()\n",
        "\n",
        "predictions_data = {\n",
        "    'Index': range(len(y_test)),\n",
        "    'Modelo 1': predictions_flat,\n",
        "    'Modelo 2': predictions_1_flat,\n",
        "    'Modelo 3': predictions_2_flat,\n",
        "    'GPA Real': y_test\n",
        "}\n",
        "\n",
        "predictions_df = pd.DataFrame(predictions_data)\n",
        "print(\"\\nTabla de Predicciones:\")\n",
        "print(predictions_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL90XOkZ_5ug",
        "outputId": "d661dbbb-9953-411b-a9c4-3a1a1ccc9152"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparación de Loss y MAE:\n",
            "     Modelo  Test Loss (MSE)  Test MAE\n",
            "0  Modelo 1         0.061366  0.196060\n",
            "1  Modelo 2         0.063572  0.204856\n",
            "2  Modelo 3         0.030593  0.194217\n",
            "\n",
            "Tabla de Predicciones:\n",
            "      Index  Modelo 1  Modelo 2  Modelo 3  GPA Real\n",
            "1004      0  1.520946  1.627941  1.831046  1.427724\n",
            "196       1  2.970200  2.873158  3.217843  3.117354\n",
            "2342      2  1.831378  1.975867  1.824360  2.037769\n",
            "1708      3  3.388228  3.215481  3.576432  3.548521\n",
            "435       4  0.396058  0.413753  0.448720  0.248977\n",
            "...     ...       ...       ...       ...       ...\n",
            "986     474  1.918678  1.841510  1.711658  1.562360\n",
            "120     475  2.169205  2.005862  1.919759  2.174903\n",
            "283     476  2.587699  2.415275  2.548670  2.332540\n",
            "1740    477  2.412328  2.373403  2.515481  2.777967\n",
            "1726    478  1.108080  0.933828  0.961009  0.863545\n",
            "\n",
            "[479 rows x 5 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}